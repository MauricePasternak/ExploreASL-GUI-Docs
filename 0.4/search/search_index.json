{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home What is ExploreASL GUI? This is a graphical user interface (GUI) meant to guide researchers and clinicians in their processing of arterial spin labeling (ASL) scans from the raw DICOM output MRI scanners to the exporting of figures and plots of cerebral blood flow (CBF) data. What are the goals of this program? To provide a performant and user-friendly experience in ASL processing without requiring any knowledge of programming or having to write error-prone configuration files. To facilitate collaboration and communication between neuroscience groups by having their ASL datasets processed in the same semi-automatic manner that is robust to multicenter and multisequence complications that would otherwise arise. To this end, the program explicitly enforces the novel ASL-BIDS extension . To minimize the probability of any user error perpetuating into a mistaken data-processing error, thereby reducing the amount of discarded neuroimaging data that enters statistical & machine-learning analyses. Every step of this program features validation of user input. Every validation gives user color & text feedback as to the nature of the error as well as suggestions for ameliorating said error, where appropriate. To promote reusable components and sensible programming logic that may be used by other neuroscience-focused developers, either in writing their own tools de novo or migrating their existing codebase for various purposes, such as \"modernizing\" the look & feel of their applications. Pipeline Overview The user interface divides the processing of ASL data into 5 sections: DICOM data coming off different MRI scanners and sites must be translated into a consistent BIDS format. Imported data should be inspected by the user as a convenient dataframe that allows for adding/deleting/editing BIDS fields for every ASL scan. For a given dataset, global parameters are defined regarding variables such as which subjects to process, CBF quantification modeling, which atlases to use for ROI analysis, etc. The ExploreASL pipeline is executed on the study in a parallel-processing manner. Specific steps or entire modules can be re-run at any level (subject, scan, etc.) Users load regional CBF values from a processed study, merge it with any clinical metadata they may have, and plot it. Interacting with the plot datapoints loads in point-specific CBF image volumes for streamlining quality-control","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#what-is-exploreasl-gui","text":"This is a graphical user interface (GUI) meant to guide researchers and clinicians in their processing of arterial spin labeling (ASL) scans from the raw DICOM output MRI scanners to the exporting of figures and plots of cerebral blood flow (CBF) data.","title":"What is ExploreASL GUI?"},{"location":"#what-are-the-goals-of-this-program","text":"To provide a performant and user-friendly experience in ASL processing without requiring any knowledge of programming or having to write error-prone configuration files. To facilitate collaboration and communication between neuroscience groups by having their ASL datasets processed in the same semi-automatic manner that is robust to multicenter and multisequence complications that would otherwise arise. To this end, the program explicitly enforces the novel ASL-BIDS extension . To minimize the probability of any user error perpetuating into a mistaken data-processing error, thereby reducing the amount of discarded neuroimaging data that enters statistical & machine-learning analyses. Every step of this program features validation of user input. Every validation gives user color & text feedback as to the nature of the error as well as suggestions for ameliorating said error, where appropriate. To promote reusable components and sensible programming logic that may be used by other neuroscience-focused developers, either in writing their own tools de novo or migrating their existing codebase for various purposes, such as \"modernizing\" the look & feel of their applications.","title":"What are the goals of this program?"},{"location":"#pipeline-overview","text":"The user interface divides the processing of ASL data into 5 sections: DICOM data coming off different MRI scanners and sites must be translated into a consistent BIDS format. Imported data should be inspected by the user as a convenient dataframe that allows for adding/deleting/editing BIDS fields for every ASL scan. For a given dataset, global parameters are defined regarding variables such as which subjects to process, CBF quantification modeling, which atlases to use for ROI analysis, etc. The ExploreASL pipeline is executed on the study in a parallel-processing manner. Specific steps or entire modules can be re-run at any level (subject, scan, etc.) Users load regional CBF values from a processed study, merge it with any clinical metadata they may have, and plot it. Interacting with the plot datapoints loads in point-specific CBF image volumes for streamlining quality-control","title":"Pipeline Overview"},{"location":"Contact/","text":"Contact For ExploreASL-GUI related questions You can contact the maintainer and developer of ExploreASL-GUI, Maurice Pasternak . For ExploreASL (backend pipeline) related questions You can contact the general ExploreASL email address with your inquiries and the appropriate person will be dispatched to answer your question. ExploreASL Team Click on any of the profile pictures below to be taken to the corresponding individual's profile. Henk Mutsaerts \ud83d\udc68\u200d\ud83d\udd2c \ud83d\udd8b \ud83d\udcbb Jan Petr \ud83d\udc68\u200d\ud83d\udd2c \ud83d\udd8b \ud83d\udcbb Michael Stritt \ud83d\udcbb \ud83d\udd8b \ud83d\udcd6 Mathijs Dijsselhof \ud83d\udd8b \ud83e\udde0 Beatriz Padrela \ud83d\udd8b \ud83e\udde0 Paul Groot \ud83d\udcbb \ud83d\udd8b Pieter Vandemaele \ud83d\udcbb \ud83e\udd14 \ud83e\udde0 MauricePasternak \ud83d\udcca \ud83d\udcbb \ud83c\udfa8 Patricia Clement \ud83e\udde0 \ud83e\udd14 \ud83d\udcd6 Sandeep Ganji \ud83d\udd8b \ud83e\udd14 \ud83e\udde0 Martin Craig \ud83d\udd8b \ud83d\udcbb \ud83e\udde0 DaveThoma5 \ud83e\udd14 \ud83e\udde0 Amnah Mahroo \ud83e\udd14 \ud83e\udde0 luislorenzini \ud83d\udcbb \ud83d\udd27 jozsait \ud83d\udcbb \ud83d\udea7","title":"Contact"},{"location":"Contact/#contact","text":"","title":"Contact"},{"location":"Contact/#for-exploreasl-gui-related-questions","text":"You can contact the maintainer and developer of ExploreASL-GUI, Maurice Pasternak .","title":"For ExploreASL-GUI related questions"},{"location":"Contact/#for-exploreasl-backend-pipeline-related-questions","text":"You can contact the general ExploreASL email address with your inquiries and the appropriate person will be dispatched to answer your question.","title":"For ExploreASL (backend pipeline) related questions"},{"location":"Contact/#exploreasl-team","text":"Click on any of the profile pictures below to be taken to the corresponding individual's profile. Henk Mutsaerts \ud83d\udc68\u200d\ud83d\udd2c \ud83d\udd8b \ud83d\udcbb Jan Petr \ud83d\udc68\u200d\ud83d\udd2c \ud83d\udd8b \ud83d\udcbb Michael Stritt \ud83d\udcbb \ud83d\udd8b \ud83d\udcd6 Mathijs Dijsselhof \ud83d\udd8b \ud83e\udde0 Beatriz Padrela \ud83d\udd8b \ud83e\udde0 Paul Groot \ud83d\udcbb \ud83d\udd8b Pieter Vandemaele \ud83d\udcbb \ud83e\udd14 \ud83e\udde0 MauricePasternak \ud83d\udcca \ud83d\udcbb \ud83c\udfa8 Patricia Clement \ud83e\udde0 \ud83e\udd14 \ud83d\udcd6 Sandeep Ganji \ud83d\udd8b \ud83e\udd14 \ud83e\udde0 Martin Craig \ud83d\udd8b \ud83d\udcbb \ud83e\udde0 DaveThoma5 \ud83e\udd14 \ud83e\udde0 Amnah Mahroo \ud83e\udd14 \ud83e\udde0 luislorenzini \ud83d\udcbb \ud83d\udd27 jozsait \ud83d\udcbb \ud83d\udea7","title":"ExploreASL Team"},{"location":"License/","text":"MIT License Copyright \u00a9 2022-2023, Maurice Pasternak Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"Requirements/","text":"Requirements Software Requirements Operating System Must be one of: Windows 10 Ubuntu 20.04 LTS or later MacOS 10.15 or later Software If using the GitHub version of ExploreASL: The GitHub-based version of ExploreASL itself. Verify that the ExploreASL version is at least 1.10.0 or newer. A working copy of MATLAB installed on your system. Version R2019a or newer is recommended. If using the Compiled version of ExploreASL: The compiled copy of ExploreASL (please contact the ExploreASL team for a copy) that is at least version 1.10.0 or newer. A working copy of MATLAB Runtime R2019a installed on your system. Note that this is subject to change depending on what version of MATLAB was used to compile ExploreASL. Ask the ExploreASL team for the clarification. Hardware Requirements 8 GB RAM (minimal; 16 GB recommended) 500 MB free disk space Relatively recent CPU (4 cores or more)","title":"Requirements"},{"location":"Requirements/#requirements","text":"","title":"Requirements"},{"location":"Requirements/#software-requirements","text":"","title":"Software Requirements"},{"location":"Requirements/#operating-system","text":"Must be one of: Windows 10 Ubuntu 20.04 LTS or later MacOS 10.15 or later","title":"Operating System"},{"location":"Requirements/#software","text":"If using the GitHub version of ExploreASL: The GitHub-based version of ExploreASL itself. Verify that the ExploreASL version is at least 1.10.0 or newer. A working copy of MATLAB installed on your system. Version R2019a or newer is recommended. If using the Compiled version of ExploreASL: The compiled copy of ExploreASL (please contact the ExploreASL team for a copy) that is at least version 1.10.0 or newer. A working copy of MATLAB Runtime R2019a installed on your system. Note that this is subject to change depending on what version of MATLAB was used to compile ExploreASL. Ask the ExploreASL team for the clarification.","title":"Software"},{"location":"Requirements/#hardware-requirements","text":"8 GB RAM (minimal; 16 GB recommended) 500 MB free disk space Relatively recent CPU (4 cores or more)","title":"Hardware Requirements"},{"location":"FAQ/FAQ/","text":"FAQ I don't have previous experience with ASL-MRI analysis. Where can I go to learn more? There is an excellent ASL overview from the creators of BASIL (an alternative to this program) on their documentation page . Come back here after you've read that. Okay, now that you understand the basics of ASL, you should be made aware that ASL imaging has been incorporated into a brain imaging standard known as BIDS . This program revolves around the ASL-BIDS extension whose specification is available in online format . Once you have given yourself a look at the above resources, you should feel more familar with the terminology that will be used by this program and the variety of inputs & labels that are found within it. If you haven't visited the tutorial section of this documentation, you are encouraged to do so by clicking here . I don't have a MATLAB license. Can I still use this program? Yes, but you will need to contact the ExploreASL team to request a compiled version of ExploreASL that does not require an official MATLAB program on your system. However, you will still need to have a MATLAB Runtime installed on your system. The particular version depends on the version of MATLAB that was used to compile the ExploreASL program. When you contact the ExploreASL team, they should be able to indicate which version of MATLAB Runtime you will need. I have ideas for new features or issues with this program. Where can I go to share these ideas/concerns/questions/etc.? This program is open source and is hosted on Github. Please see the Issues page to submit a Bug Report or Feature Request. If you are not familiar with Github, you can also contact the developer of the program to share your ideas/concerns. Note: This is primarly for issues associated with the graphical user interface (GUI), not with the \"backend\" ExploreASL program. While it can be difficult to differentiate the origin of issues that arise (i.e. was it the GUI that failed or is it an issues with ExploreASL), a general rule of thumb is that if there is a message that appears with reference to a folder location of \"derivatives/ExploreASL\" , then it is likely an issue with ExploreASL itself. If there is no such message, then it is likely an issue with the GUI.","title":"FAQ"},{"location":"FAQ/FAQ/#faq","text":"I don't have previous experience with ASL-MRI analysis. Where can I go to learn more? There is an excellent ASL overview from the creators of BASIL (an alternative to this program) on their documentation page . Come back here after you've read that. Okay, now that you understand the basics of ASL, you should be made aware that ASL imaging has been incorporated into a brain imaging standard known as BIDS . This program revolves around the ASL-BIDS extension whose specification is available in online format . Once you have given yourself a look at the above resources, you should feel more familar with the terminology that will be used by this program and the variety of inputs & labels that are found within it. If you haven't visited the tutorial section of this documentation, you are encouraged to do so by clicking here . I don't have a MATLAB license. Can I still use this program? Yes, but you will need to contact the ExploreASL team to request a compiled version of ExploreASL that does not require an official MATLAB program on your system. However, you will still need to have a MATLAB Runtime installed on your system. The particular version depends on the version of MATLAB that was used to compile the ExploreASL program. When you contact the ExploreASL team, they should be able to indicate which version of MATLAB Runtime you will need. I have ideas for new features or issues with this program. Where can I go to share these ideas/concerns/questions/etc.? This program is open source and is hosted on Github. Please see the Issues page to submit a Bug Report or Feature Request. If you are not familiar with Github, you can also contact the developer of the program to share your ideas/concerns. Note: This is primarly for issues associated with the graphical user interface (GUI), not with the \"backend\" ExploreASL program. While it can be difficult to differentiate the origin of issues that arise (i.e. was it the GUI that failed or is it an issues with ExploreASL), a general rule of thumb is that if there is a message that appears with reference to a folder location of \"derivatives/ExploreASL\" , then it is likely an issue with ExploreASL itself. If there is no such message, then it is likely an issue with the GUI.","title":"FAQ"},{"location":"Installation/Developers/","text":"Developer Installation This section is for developers who want to contribute to the development of the software. If you are a user, please refer to the User Installation section. As with the user installation, there are 3 components required for working with this software: - The graphical user interface (GUI) software itself (this program) - An ExploreASL version (compiled or from GitHub) - A MATLAB version (regular or MATLAB Runtime) Instructions for obtaining the latter two were outlied in the User Installation section. Prerequisites Before fetching the source code for the GUI, you will need to install the LTS version of Node.js . This is because the GUI's backend uses Node.js to run the ExploreASL pipeline. In addition, you will need a package manager for getting all the dependencies for the GUI. The GUI uses Yarn as its package manager. You can install Yarn via the installation instructions on their website. Downloading the Source Code Instead of heading over to the Releases page on the GitHub repository of this GUI, you will need to clone the repository via command: git clone https://github.com/MauricePasternak/ExploreASL-GUI.git Installing Dependencies Once you have the source code, you will need to install all the dependencies for the GUI. First navigate to the directory where you cloned the repository: cd path/to/ExploreASL-GUI Then install all the dependencies: yarn install Running the GUI Once you have the dependencies installed, you can run the GUI via command: yarn start And that's it! You should now be able to see the GUI running on your computer as two windows: one for the GUI itself and the console/debugger window for assistance with development. As you make changes to the source code, the GUI will automatically reload. Note that this only applies to changes made to the frontend UI portion. Changes to the backend will require you to close the GUI and restart it for code changes to be reflected. Packaging/Deploying the GUI If you want to package the GUI for distribution, you can do so via command: yarn make This will create a folder in the root directory of the repository called out . This folder will contain the packaged GUI for your operating system. A Note about the Project's Structure The GUI's complexity requires a certain level of organization to keep things manageable. The following is a brief overview of the project's structure and how it is organized: /---| |-> assets (media such as icons) |-> backend (logic executed by electron's IpcMain, i.e. spawning subprocesses to run ExploreASL) |-> common -| | |-> schemas (form validators & form schemas) | |-> types (reusable types & typescript declarations) | |-> utilityFunctions (reusable functions for reducing code use) | |-> GLOBALS.ts (global variables used throughout) | |-> ipc (logic for type-safe IpcMain <-> IpcRenderer communication) |-> components (reuseable React components) |-> pages (non-reuseable React components that make up the pages of the GUI) |-> stores (frontend-only collections of user-interface state) ... other files relate to project setup, package-handling, etc.","title":"For Developers"},{"location":"Installation/Developers/#developer-installation","text":"This section is for developers who want to contribute to the development of the software. If you are a user, please refer to the User Installation section. As with the user installation, there are 3 components required for working with this software: - The graphical user interface (GUI) software itself (this program) - An ExploreASL version (compiled or from GitHub) - A MATLAB version (regular or MATLAB Runtime) Instructions for obtaining the latter two were outlied in the User Installation section.","title":"Developer Installation"},{"location":"Installation/Developers/#prerequisites","text":"Before fetching the source code for the GUI, you will need to install the LTS version of Node.js . This is because the GUI's backend uses Node.js to run the ExploreASL pipeline. In addition, you will need a package manager for getting all the dependencies for the GUI. The GUI uses Yarn as its package manager. You can install Yarn via the installation instructions on their website.","title":"Prerequisites"},{"location":"Installation/Developers/#downloading-the-source-code","text":"Instead of heading over to the Releases page on the GitHub repository of this GUI, you will need to clone the repository via command: git clone https://github.com/MauricePasternak/ExploreASL-GUI.git","title":"Downloading the Source Code"},{"location":"Installation/Developers/#installing-dependencies","text":"Once you have the source code, you will need to install all the dependencies for the GUI. First navigate to the directory where you cloned the repository: cd path/to/ExploreASL-GUI Then install all the dependencies: yarn install","title":"Installing Dependencies"},{"location":"Installation/Developers/#running-the-gui","text":"Once you have the dependencies installed, you can run the GUI via command: yarn start And that's it! You should now be able to see the GUI running on your computer as two windows: one for the GUI itself and the console/debugger window for assistance with development. As you make changes to the source code, the GUI will automatically reload. Note that this only applies to changes made to the frontend UI portion. Changes to the backend will require you to close the GUI and restart it for code changes to be reflected.","title":"Running the GUI"},{"location":"Installation/Developers/#packagingdeploying-the-gui","text":"If you want to package the GUI for distribution, you can do so via command: yarn make This will create a folder in the root directory of the repository called out . This folder will contain the packaged GUI for your operating system.","title":"Packaging/Deploying the GUI"},{"location":"Installation/Developers/#a-note-about-the-projects-structure","text":"The GUI's complexity requires a certain level of organization to keep things manageable. The following is a brief overview of the project's structure and how it is organized: /---| |-> assets (media such as icons) |-> backend (logic executed by electron's IpcMain, i.e. spawning subprocesses to run ExploreASL) |-> common -| | |-> schemas (form validators & form schemas) | |-> types (reusable types & typescript declarations) | |-> utilityFunctions (reusable functions for reducing code use) | |-> GLOBALS.ts (global variables used throughout) | |-> ipc (logic for type-safe IpcMain <-> IpcRenderer communication) |-> components (reuseable React components) |-> pages (non-reuseable React components that make up the pages of the GUI) |-> stores (frontend-only collections of user-interface state) ... other files relate to project setup, package-handling, etc.","title":"A Note about the Project's Structure"},{"location":"Installation/Users/","text":"User Installation There are essentially 3 components required for ASL Analysis using this software: The graphical user interface (GUI) software itself (this program) An ExploreASL version (compiled or from GitHub) A MATLAB version (regular or MATLAB Runtime) Let's cover the steps for getting each one: GUI Simple enough, head on over to the Releases page on the GitHub repository of this GUI and download the version of your preference. The latest version is recommended. ExploreASL There are 2 versions of ExploreASL available: From GitHub Compiled These will be referenced regularly within the GUI itself, so be sure to know which one you're using. From GitHub Getting the GitHub one is straight forward. Head on over to the ExploreASL repository and either use the git command line tool to clone the repository via command: git clone https://github.com/ExploreASL/ExploreASL.git Or download the repository as a ZIP file by clicking the green Code button and selecting Download ZIP . Compiled Getting the compiled version is not as straight forward. You will need to contact the ExploreASL team and request a copy of the compiled version. Be sure to take note which MATLAB version was used to create this compiled package. MATLAB There are 2 versions of MATLAB available: Regular MATLAB Runtime Regular MATLAB This is a requirement if you are using the GitHub version of ExploreASL. You can download the latest version of MATLAB from the MathWorks website . MATLAB Runtime This is a requirement if you are using the compiled version of ExploreASL. You can download MATLAB Runtime from the MathWorks website . Be sure to download the version that matches the version of MATLAB used to compile the ExploreASL package.","title":"For Users"},{"location":"Installation/Users/#user-installation","text":"There are essentially 3 components required for ASL Analysis using this software: The graphical user interface (GUI) software itself (this program) An ExploreASL version (compiled or from GitHub) A MATLAB version (regular or MATLAB Runtime) Let's cover the steps for getting each one:","title":"User Installation"},{"location":"Installation/Users/#gui","text":"Simple enough, head on over to the Releases page on the GitHub repository of this GUI and download the version of your preference. The latest version is recommended.","title":"GUI"},{"location":"Installation/Users/#exploreasl","text":"There are 2 versions of ExploreASL available: From GitHub Compiled These will be referenced regularly within the GUI itself, so be sure to know which one you're using.","title":"ExploreASL"},{"location":"Installation/Users/#from-github","text":"Getting the GitHub one is straight forward. Head on over to the ExploreASL repository and either use the git command line tool to clone the repository via command: git clone https://github.com/ExploreASL/ExploreASL.git Or download the repository as a ZIP file by clicking the green Code button and selecting Download ZIP .","title":"From GitHub"},{"location":"Installation/Users/#compiled","text":"Getting the compiled version is not as straight forward. You will need to contact the ExploreASL team and request a copy of the compiled version. Be sure to take note which MATLAB version was used to create this compiled package.","title":"Compiled"},{"location":"Installation/Users/#matlab","text":"There are 2 versions of MATLAB available: Regular MATLAB Runtime","title":"MATLAB"},{"location":"Installation/Users/#regular-matlab","text":"This is a requirement if you are using the GitHub version of ExploreASL. You can download the latest version of MATLAB from the MathWorks website .","title":"Regular MATLAB"},{"location":"Installation/Users/#matlab-runtime","text":"This is a requirement if you are using the compiled version of ExploreASL. You can download MATLAB Runtime from the MathWorks website . Be sure to download the version that matches the version of MATLAB used to compile the ExploreASL package.","title":"MATLAB Runtime"},{"location":"Tutorial/0_Preface/Starting_Page/","text":"Starting Page Overview When you start up ExploreASL-GUI, you will be greeted with the following screen: This is the About Page. It contains information about the program, some rephrasing of what is mentioned in this documentation, links and contacts, etc. Toolbar The toolbar is located at the top of the GUI. It contains the following buttons: For a more modern look, you can toggle a dark theme to keep the GUI in sync with your operating system. When you restart the GUI, this choice will be remembered. Agnostic to operating system, the GUI's window controls (minimize, maximize, close) are located in the top right corner of the GUI. Apologies to the MacOS users for whom this is a deal-breaker. And finally, to navigate to the different pages of the GUI, there is a menu/navigation toggle located in the top left corner of the GUI. Navigation Toggling the navigation menu will open a sidebar drawer that contains links to the different pages of the GUI: Each link corresponds to a different section/focus of the GUI. In a nutshell: Import a Dataset , for importing your data from DICOMs to NIfTI files Define Parameters , for defining the plethora of parameters that ExploreASL uses to process your data Edit BIDS Sidecars , for fine-tuned adjustments to the BIDS sidecars that are generated during the import process, allowing you to specify the exact parameters that apply for a particular scan, as these may have been missed by the import process due to overly-liberal anonymization procedures. Process Studies , for analyzing your imported and adjusted data in a multiprocessing fashion Data Visualization , for visualizing your processed ASL dataset in combination with your own metadata (i.e. demographics, clinical data, etc.) With all of this in mind, let's proceed to import our data into NIftI format.","title":"Starting Page"},{"location":"Tutorial/0_Preface/Starting_Page/#starting-page","text":"","title":"Starting Page"},{"location":"Tutorial/0_Preface/Starting_Page/#overview","text":"When you start up ExploreASL-GUI, you will be greeted with the following screen: This is the About Page. It contains information about the program, some rephrasing of what is mentioned in this documentation, links and contacts, etc.","title":"Overview"},{"location":"Tutorial/0_Preface/Starting_Page/#toolbar","text":"The toolbar is located at the top of the GUI. It contains the following buttons: For a more modern look, you can toggle a dark theme to keep the GUI in sync with your operating system. When you restart the GUI, this choice will be remembered. Agnostic to operating system, the GUI's window controls (minimize, maximize, close) are located in the top right corner of the GUI. Apologies to the MacOS users for whom this is a deal-breaker. And finally, to navigate to the different pages of the GUI, there is a menu/navigation toggle located in the top left corner of the GUI.","title":"Toolbar"},{"location":"Tutorial/0_Preface/Starting_Page/#navigation","text":"Toggling the navigation menu will open a sidebar drawer that contains links to the different pages of the GUI: Each link corresponds to a different section/focus of the GUI. In a nutshell: Import a Dataset , for importing your data from DICOMs to NIfTI files Define Parameters , for defining the plethora of parameters that ExploreASL uses to process your data Edit BIDS Sidecars , for fine-tuned adjustments to the BIDS sidecars that are generated during the import process, allowing you to specify the exact parameters that apply for a particular scan, as these may have been missed by the import process due to overly-liberal anonymization procedures. Process Studies , for analyzing your imported and adjusted data in a multiprocessing fashion Data Visualization , for visualizing your processed ASL dataset in combination with your own metadata (i.e. demographics, clinical data, etc.) With all of this in mind, let's proceed to import our data into NIftI format.","title":"Navigation"},{"location":"Tutorial/0_Preface/Tutorials_Dataset/","text":"Preface For this tutorial, it is assumed you have: Met the requirements for running the software Have installed the software as well as its MATLAB and ExploreASL dependencies Example Dataset Introduction Throughout this tutorial, we will be using the following dataset as an example: If you've done your homework, this should look like the start of a BIDS dataset. If not, please familizarize yourself with the BIDS specification . Throughout the GUI, there will be requests to provide a \"Study Root Directory\". In this example, this would be the folder called MyStudyRoot . This is the folder that, itself, will contain the BIDS-compliant subfolders: sourcedata ; where your raw DICOMs are store rawdata ; where your DICOMs are converted to NIfTI files derivatives ; where the ASL processing is performed For your own project, you will want to ensure at the very least that you have a sourcedata folder located under your own Study Root Directory . Also, take note at the consistency of the folder structure that comes after sourcedata . This is a requirement for the GUI to be able to properly import your data from DICOMs to NIfTI files. This will be covered in further detail in the \" Define Filepaths and Folder Structure \" section of this tutorial. For now, just take note that the level after sourcedata features folders corresponding to individual subject. In fact, there are three different ways that these subjects were acquired which leads to the different naming suffix here: 001GE and 002GE were subjects whose scans were performed on a GE scanner with a 3D Spiral sequence and PCASL labeling. 003Siemens20 was scanned on a Siemens scanner with 3D GRASE sequence and PASL labeling, and output as 20 DICOMs representing 10 control & label pairs. 004Siemens60 was acquired on a Siemens with PASL labeling, and output as 60 DICOMs representing an averaged control & label pair. Inside each of these subject folders, there are two subfolders: 01 , representing scans from the baseline visit 02 , representing scans from the follow-up visit Finally, inside these visit folders are the scan folders: ASL , containing the ASL data (or possibly an embedded M0 scan within, as is the case for the GE data here) T1 , containing the T1-weighted structural data M0 , for the proton density weighted image needed for normalizing perfusion weighted images into absolute units Note that these folder names are not absolutely necessary. They could have been called \"FOO\", \"BAR\", etc. As long as the folder names are consistent, you will be able to instruct the ExploreASL-GUI on what is what.","title":"Tutorial's Dataset"},{"location":"Tutorial/0_Preface/Tutorials_Dataset/#preface","text":"For this tutorial, it is assumed you have: Met the requirements for running the software Have installed the software as well as its MATLAB and ExploreASL dependencies","title":"Preface"},{"location":"Tutorial/0_Preface/Tutorials_Dataset/#example-dataset-introduction","text":"Throughout this tutorial, we will be using the following dataset as an example: If you've done your homework, this should look like the start of a BIDS dataset. If not, please familizarize yourself with the BIDS specification . Throughout the GUI, there will be requests to provide a \"Study Root Directory\". In this example, this would be the folder called MyStudyRoot . This is the folder that, itself, will contain the BIDS-compliant subfolders: sourcedata ; where your raw DICOMs are store rawdata ; where your DICOMs are converted to NIfTI files derivatives ; where the ASL processing is performed For your own project, you will want to ensure at the very least that you have a sourcedata folder located under your own Study Root Directory . Also, take note at the consistency of the folder structure that comes after sourcedata . This is a requirement for the GUI to be able to properly import your data from DICOMs to NIfTI files. This will be covered in further detail in the \" Define Filepaths and Folder Structure \" section of this tutorial. For now, just take note that the level after sourcedata features folders corresponding to individual subject. In fact, there are three different ways that these subjects were acquired which leads to the different naming suffix here: 001GE and 002GE were subjects whose scans were performed on a GE scanner with a 3D Spiral sequence and PCASL labeling. 003Siemens20 was scanned on a Siemens scanner with 3D GRASE sequence and PASL labeling, and output as 20 DICOMs representing 10 control & label pairs. 004Siemens60 was acquired on a Siemens with PASL labeling, and output as 60 DICOMs representing an averaged control & label pair. Inside each of these subject folders, there are two subfolders: 01 , representing scans from the baseline visit 02 , representing scans from the follow-up visit Finally, inside these visit folders are the scan folders: ASL , containing the ASL data (or possibly an embedded M0 scan within, as is the case for the GE data here) T1 , containing the T1-weighted structural data M0 , for the proton density weighted image needed for normalizing perfusion weighted images into absolute units Note that these folder names are not absolutely necessary. They could have been called \"FOO\", \"BAR\", etc. As long as the folder names are consistent, you will be able to instruct the ExploreASL-GUI on what is what.","title":"Example Dataset Introduction"},{"location":"Tutorial/1_Import/0_Overview/","text":"Import Module Overview In this tutorial, we will be importing our dataset's DICOMs into NIfTI files. This is a necessary prerequisite for the rest of the pipeline. The necessity for this stems from the consensus of using NIfTI files as the standard format for neuroimaging data analysis by the scientific community. There have been several excellent tools developed for this purpose, such as dcm2niix , dcm2bids , among others. Unfortunately, these tools are either not fully automated, or require a significant degree of programming knowledge to use. ExploreASL-GUI will avoid these caveats by allowing you to specify the meanings of folders found in your dataset and then executing the import process in a fully automated fashion. Of course, the drawback is that your dataset must follow certain rules, which we will discuss in the next section when we look at the first page of the Import pipeline.","title":"Module Overview"},{"location":"Tutorial/1_Import/0_Overview/#import-module-overview","text":"In this tutorial, we will be importing our dataset's DICOMs into NIfTI files. This is a necessary prerequisite for the rest of the pipeline. The necessity for this stems from the consensus of using NIfTI files as the standard format for neuroimaging data analysis by the scientific community. There have been several excellent tools developed for this purpose, such as dcm2niix , dcm2bids , among others. Unfortunately, these tools are either not fully automated, or require a significant degree of programming knowledge to use. ExploreASL-GUI will avoid these caveats by allowing you to specify the meanings of folders found in your dataset and then executing the import process in a fully automated fashion. Of course, the drawback is that your dataset must follow certain rules, which we will discuss in the next section when we look at the first page of the Import pipeline.","title":"Import Module Overview"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/","text":"Defining Filepaths and Folder Structure ExploreASL and MATLAB Paths The import process requires knowledge about the underlying ExploreASL program and MATLAB. You will have to indicate: ExploreASL Type : whether you are using the GitHub version or the compiled version of ExploreASL. ExploreASL Path : the path to the ExploreASL main folder, which should be either called ExploreASL or xASL-latest for the GitHub and compiled versions, respectively. MATLAB Runtime Path : the path to the MATLAB Runtime folder, which should have a folder name along the lines of v96 , v97 , etc. This is only required if you are using the compiled version of ExploreASL. For clarification, here is what the structure of the ExploreASL from GitHub should look like: And where is what the xASL-latest folder should look like for the compiled version of ExploreASL: Folder Structure Before we discuss the folder structure, let's first fill in the Study Root Directory field. Recall that this is the folder that will contain the BIDS-compliant subfolders: sourcedata rawdata derivatives Proper Folder Structure Recall that the overview for this section mentioned that the GUI requires that your DICOM folder structure follow certain rules. The rules are as follows: Compartmentalization : Every child, grandchild, etc. of the sourcedata folder must only contain 1 piece of information . A piece of information is one of the following: - A subject's designation - A visit's designation - A session's designation - A scan's designation The following is an example of where this rule is violated: The issue here is that child folders of sourcedata have 2 pieces of information: the subject and the visit. Consistent Ordering : The same piece of information must always be found in the Nth depth down from sourcedata . Fortunately, this rule is very difficult to violate, as the output of MRI scanners generally follows a consistent ordering. The following is an example of where this rule is violated: The issue here is that in some cases, a subject is found as the grandchild of sourcedata , while in other cases, a subject is found as a direct child of sourcedata . Same goes for the visit designations. Consistent Naming : For visits and scans, the same piece of text should describe that temporal aspect of the data. For example, if a baseline visit is called \"Baseline\" for one subject, it should be called \"Baseline\" for all subjects. The following is an example of where this rule is violated: The issue here is that there is a back-and-forth between using the numerical designation of Baseline and Followup versus the explicit text representation of those visit names. Specifying the Folder Structure Now that we have discussed the rules for the folder structure, let's discuss how to specify the folder structure in the GUI. The GUI will request two pieces of information: The number of folder levels that are between sourcedata and the DICOM files. If we look into our project folder, we can see that there are 3 levels between sourcedata and the DICOM files: Therefore, we will enter 3 into the Number of levels between sourcedata and DICOM files field. What is found at each directory level. There are 5 options available: Subject Visit Session (sometimes called \"Run\" in literature) Scan Ignore That last option is for folder levels that don't offer any valuable information. For example, some scanner typically have a folder called DICOM that contains either additional folders or the DICOM files themselves. This folder is not useful for the import process, so we can ignore it. At this point, no validation errors should be present. We can proceed to the next step by clicking the Next button located in the bottom right of the program.","title":"Define Filepaths and Folder Structure"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#defining-filepaths-and-folder-structure","text":"","title":"Defining Filepaths and Folder Structure"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#exploreasl-and-matlab-paths","text":"The import process requires knowledge about the underlying ExploreASL program and MATLAB. You will have to indicate: ExploreASL Type : whether you are using the GitHub version or the compiled version of ExploreASL. ExploreASL Path : the path to the ExploreASL main folder, which should be either called ExploreASL or xASL-latest for the GitHub and compiled versions, respectively. MATLAB Runtime Path : the path to the MATLAB Runtime folder, which should have a folder name along the lines of v96 , v97 , etc. This is only required if you are using the compiled version of ExploreASL. For clarification, here is what the structure of the ExploreASL from GitHub should look like: And where is what the xASL-latest folder should look like for the compiled version of ExploreASL:","title":"ExploreASL and MATLAB Paths"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#folder-structure","text":"Before we discuss the folder structure, let's first fill in the Study Root Directory field. Recall that this is the folder that will contain the BIDS-compliant subfolders: sourcedata rawdata derivatives","title":"Folder Structure"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#proper-folder-structure","text":"Recall that the overview for this section mentioned that the GUI requires that your DICOM folder structure follow certain rules. The rules are as follows:","title":"Proper Folder Structure"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#compartmentalization","text":"Every child, grandchild, etc. of the sourcedata folder must only contain 1 piece of information . A piece of information is one of the following: - A subject's designation - A visit's designation - A session's designation - A scan's designation The following is an example of where this rule is violated: The issue here is that child folders of sourcedata have 2 pieces of information: the subject and the visit.","title":"Compartmentalization:"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#consistent-ordering","text":"The same piece of information must always be found in the Nth depth down from sourcedata . Fortunately, this rule is very difficult to violate, as the output of MRI scanners generally follows a consistent ordering. The following is an example of where this rule is violated: The issue here is that in some cases, a subject is found as the grandchild of sourcedata , while in other cases, a subject is found as a direct child of sourcedata . Same goes for the visit designations.","title":"Consistent Ordering:"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#consistent-naming","text":"For visits and scans, the same piece of text should describe that temporal aspect of the data. For example, if a baseline visit is called \"Baseline\" for one subject, it should be called \"Baseline\" for all subjects. The following is an example of where this rule is violated: The issue here is that there is a back-and-forth between using the numerical designation of Baseline and Followup versus the explicit text representation of those visit names.","title":"Consistent Naming:"},{"location":"Tutorial/1_Import/1_Defining_Folder_Structure/#specifying-the-folder-structure","text":"Now that we have discussed the rules for the folder structure, let's discuss how to specify the folder structure in the GUI. The GUI will request two pieces of information: The number of folder levels that are between sourcedata and the DICOM files. If we look into our project folder, we can see that there are 3 levels between sourcedata and the DICOM files: Therefore, we will enter 3 into the Number of levels between sourcedata and DICOM files field. What is found at each directory level. There are 5 options available: Subject Visit Session (sometimes called \"Run\" in literature) Scan Ignore That last option is for folder levels that don't offer any valuable information. For example, some scanner typically have a folder called DICOM that contains either additional folders or the DICOM files themselves. This folder is not useful for the import process, so we can ignore it. At this point, no validation errors should be present. We can proceed to the next step by clicking the Next button located in the bottom right of the program.","title":"Specifying the Folder Structure"},{"location":"Tutorial/1_Import/2_Defining_Aliases/","text":"Defining Aliases There are two issues we need to resolve here. Allowing the import pipeline to understand what folder names mean. Our example may have convenient names for scans like \"ASL\" or \"T1\", but this is not always the case. Sometimes it may be a random set of characters that is consistent across all subjects/visits/etc. Allowing for renaming from these difficult-to-understand DICOM folder names to more legible and shareable aliases. Defining Aliases for Scans The first problem is tackled when we define aliases for scans. In the earlier step, when we specified which folder depth had \" Scan \"s in it, the program has now gathered the folder names present at that level so that we can define the meaning behind them. The options are as follows: Ignore this Folder : This option is useful for additional fluff output by the scanner that happens to be present at the same level as the scans. T1 Structural Scans T2 Structural Scans FLAIR Structural Scans ASL Functional Scans Proton-Density (M0) Scans In our example, the mapping is obvious. Again, this is not always the case. Defining Aliases for Visits and Sessions The second problem is tackled by defining aliases for visits and sessions. The same logic applies as for scans: from our earlier step, the program understands the names of folders found at a certain depth away from sourcedata . Now it is asking the question \"would you like this to be called something else when its imported?\". In our example, this would be advantageous. The folder names of 01 and 02 are not very descriptive of their meaning (i.e. that one is a baseline scan and the other is a followup). Therefore we can specify that these should be called \"Baseline\" and \"Followup\" respectively. You can also see that when a particular piece of information is absent (in our example, Session/Run information), a default description is placed in the position where a mapping input would be. We can proceed to the penultimate step of the import pipeline.","title":"Define Aliases"},{"location":"Tutorial/1_Import/2_Defining_Aliases/#defining-aliases","text":"There are two issues we need to resolve here. Allowing the import pipeline to understand what folder names mean. Our example may have convenient names for scans like \"ASL\" or \"T1\", but this is not always the case. Sometimes it may be a random set of characters that is consistent across all subjects/visits/etc. Allowing for renaming from these difficult-to-understand DICOM folder names to more legible and shareable aliases.","title":"Defining Aliases"},{"location":"Tutorial/1_Import/2_Defining_Aliases/#defining-aliases-for-scans","text":"The first problem is tackled when we define aliases for scans. In the earlier step, when we specified which folder depth had \" Scan \"s in it, the program has now gathered the folder names present at that level so that we can define the meaning behind them. The options are as follows: Ignore this Folder : This option is useful for additional fluff output by the scanner that happens to be present at the same level as the scans. T1 Structural Scans T2 Structural Scans FLAIR Structural Scans ASL Functional Scans Proton-Density (M0) Scans In our example, the mapping is obvious. Again, this is not always the case.","title":"Defining Aliases for Scans"},{"location":"Tutorial/1_Import/2_Defining_Aliases/#defining-aliases-for-visits-and-sessions","text":"The second problem is tackled by defining aliases for visits and sessions. The same logic applies as for scans: from our earlier step, the program understands the names of folders found at a certain depth away from sourcedata . Now it is asking the question \"would you like this to be called something else when its imported?\". In our example, this would be advantageous. The folder names of 01 and 02 are not very descriptive of their meaning (i.e. that one is a baseline scan and the other is a followup). Therefore we can specify that these should be called \"Baseline\" and \"Followup\" respectively. You can also see that when a particular piece of information is absent (in our example, Session/Run information), a default description is placed in the position where a mapping input would be. We can proceed to the penultimate step of the import pipeline.","title":"Defining Aliases for Visits and Sessions"},{"location":"Tutorial/1_Import/3_Defining_Contexts/","text":"Define Contexts What is a context? A \"context\" here refers to the particular unique settings that defines a grouping of ASL image acquisitions. Factors like which vendor, scanner, sequence, labeling strategy, number of control-label pairs, etc. are all factors that can be used to define a context. Goodness willing, you will only have to define one context if your dataset is relatively simple (i.e. single scanner at a single site). However, for complex datasets, it is likely that you will have to define multiple contexts. What are the contexts here? In the example dataset, we have three different contexts: Subjects 001 and 002 were acquired on a GE scanner with a 3D Spiral sequence and PCASL labeling. Subject 003 was acquired on a Siemens scanner with 3D GRASE sequence and PASL labeling, and output as 20 DICOMs representing 10 control & label pairs. Subject 004 was acquired on a Siemens scanner with PASL labeling, and output as 60 DICOMs representing an averaged control & label pair. Despite being called \"Global Context\", it won't apply to the entire dataset if other contexts (like in this case) are present. It will only apply to the subjects/visits/sessions that are NOT specified in some other context. Parts of a context For the most optimal processability, the program will require information about the following aspects of your data: ASL Context : what does the ASL data look like (i.e. is it a control-label alterating series, a pre-processed perfusion image, etc.)? M0 Scan Information : is there an M0 scan, and if so, where is it located? Or should a numerical estimate be used instead? Additional ASL Information : properties like the scanner's manufacturer, ASL labeling type, etc. Background Suppression Information : is there background suppression, and if so, what are the timings? ASL Context The ASL Context is a mandatory set of fields that, together, allows the program to understand the nature of the ASL data. Otherwise, it would be tedious to manually specify what each volume in an ASL timeseries represents, especially if it consists of many volumes with embedded dummy scans. The fields to fill out are: The ASL Series Pattern: A general template to understand the type of ASL data present. Options include: Alternating Control, Label Series : This is the most common type of ASL data. It is a series of alternating control and label volumes, with the first volume being a control volume. Alternating Label, Control Series : This is the same as the previous option, but with the first volume being a label volume. Intermediate Perfusion Weighted Image : This is a pre-processed difference image between the control and label volumes. Complete Perfusion (CBF) Image : This is a pre-processed CBF image. Number of Volumes in the ASL Series: The number of volumes present overall in the series, inclusive of dummy and M0 scans that may be present. M0 Positions within ASL Series: The locations, as numbers separated by commas, of where embedded M0 scans are located within the ASL series. If there are no M0 scans, leave this field blank. 1 is the first volume in the series. Dummy Scan Positions within ASL Series: Same logic as the previous field, but for dummy scans. For the example dataset, where we are leaving the GE data as the \"global\" context, we have knowledge that the first volume (of 2) is a perfusion weighted (difference) image, and the second volume is the M0 scan. So we fill out the fields as follows: M0 Scan Information These are the fields that allow the program to understand where the M0 scan is located, and how it should be processed. The fields to fill out are: M0 Type: The nature of the M0 scan itself. Options include: Separate : The M0 scan is a separate acquisition from the ASL scan. It is not embedded within the ASL series. Included : The M0 scan is embedded within the ASL scan. Estimate : The M0 scan is not present, and in its place a numerical estimate should be used. Absent : The M0 scan is not present, and no numerical estimate should be used (not recommended). M0 Estimate: If the previous field is set to Estimate , then this field is required. It is the numerical value that will be used in place of the M0 scan in CBF quantification. Additional ASL Information These are fields that allow the program to understand the properties of the ASL data. The fields to fill out are: Manufacturer: The manufacturer of the scanner that acquired the data. Options include: GE Philips Siemens ASL Sequence Type: The sequence used to acquire the data. Options include: 2D EPI 3D Spiral 3D GRASE Labeling Type: The type of ASL labeling strategy used. Options include: PCASL PASL CASL Post Label Delay: For PASL scans or GE 3D Spiral sequences, the value to use is that of TI (inversion time). For (P)CASL scans, it is the time between the end of the labeling period and the start of the MRI acquisition. Labeling Duration: The duration of the labeling pulse, in milliseconds. This should be left alone at zero if the labeling type is PASL Bolus Cut-Off Flag: This is the PASL-only field that indicates whether the bolus cut-off technique was used (recommended for accurate quantification of PASL data). Bolus Cut-Off Technique: This is the PASL-only field that indicates the type of bolus cut-off technique used. Options include: QUIPSS , QUIPSSII , Q2TIPS Bolus Cut-Off Delay Time: This is the PASL-only field that indicates the timings used to define the labeling period. For Q2TIPs, the field change to 2 timings for the first and last bolus cut-off pulses, respectively. Background Suppression Information These are fields that assist with correcting for the effect of background suppression on the ASL data. The fields to fill out are: - Number of Background Suppression Pulses: The number of background suppression pulses present in the ASL scan. - Background Suppression Pulses Timing: The timings of the background suppression pulses, in seconds, separated by commas. Here is an example of the fields filled out for the GE \"global context\" data: Additional Context - Specifying Subsets Each \"Additional Context\" interface has an extra field featuring a filepath dropzone for identifying which subjects/visits/sessions are to be treated with that context. Note that folders that were earlier specified as Ignore or Scan when defining the dataset structure will not be allowed in the dropzone. Reloading upon revisit Since this step is by far the most tedious, a series of configuration files are saved at the study root folder: These will be used to auto-reload the contexts if this step is revisited for the same study. Note that this assumes that the dataset structure has not changed since the last time the step was completed. Once all the contexts have been defined, click Next in the bottom right corner to proceed to the next step.","title":"Define Contexts"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#define-contexts","text":"","title":"Define Contexts"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#what-is-a-context","text":"A \"context\" here refers to the particular unique settings that defines a grouping of ASL image acquisitions. Factors like which vendor, scanner, sequence, labeling strategy, number of control-label pairs, etc. are all factors that can be used to define a context. Goodness willing, you will only have to define one context if your dataset is relatively simple (i.e. single scanner at a single site). However, for complex datasets, it is likely that you will have to define multiple contexts.","title":"What is a context?"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#what-are-the-contexts-here","text":"In the example dataset, we have three different contexts: Subjects 001 and 002 were acquired on a GE scanner with a 3D Spiral sequence and PCASL labeling. Subject 003 was acquired on a Siemens scanner with 3D GRASE sequence and PASL labeling, and output as 20 DICOMs representing 10 control & label pairs. Subject 004 was acquired on a Siemens scanner with PASL labeling, and output as 60 DICOMs representing an averaged control & label pair. Despite being called \"Global Context\", it won't apply to the entire dataset if other contexts (like in this case) are present. It will only apply to the subjects/visits/sessions that are NOT specified in some other context.","title":"What are the contexts here?"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#parts-of-a-context","text":"For the most optimal processability, the program will require information about the following aspects of your data: ASL Context : what does the ASL data look like (i.e. is it a control-label alterating series, a pre-processed perfusion image, etc.)? M0 Scan Information : is there an M0 scan, and if so, where is it located? Or should a numerical estimate be used instead? Additional ASL Information : properties like the scanner's manufacturer, ASL labeling type, etc. Background Suppression Information : is there background suppression, and if so, what are the timings?","title":"Parts of a context"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#asl-context","text":"The ASL Context is a mandatory set of fields that, together, allows the program to understand the nature of the ASL data. Otherwise, it would be tedious to manually specify what each volume in an ASL timeseries represents, especially if it consists of many volumes with embedded dummy scans. The fields to fill out are: The ASL Series Pattern: A general template to understand the type of ASL data present. Options include: Alternating Control, Label Series : This is the most common type of ASL data. It is a series of alternating control and label volumes, with the first volume being a control volume. Alternating Label, Control Series : This is the same as the previous option, but with the first volume being a label volume. Intermediate Perfusion Weighted Image : This is a pre-processed difference image between the control and label volumes. Complete Perfusion (CBF) Image : This is a pre-processed CBF image. Number of Volumes in the ASL Series: The number of volumes present overall in the series, inclusive of dummy and M0 scans that may be present. M0 Positions within ASL Series: The locations, as numbers separated by commas, of where embedded M0 scans are located within the ASL series. If there are no M0 scans, leave this field blank. 1 is the first volume in the series. Dummy Scan Positions within ASL Series: Same logic as the previous field, but for dummy scans. For the example dataset, where we are leaving the GE data as the \"global\" context, we have knowledge that the first volume (of 2) is a perfusion weighted (difference) image, and the second volume is the M0 scan. So we fill out the fields as follows:","title":"ASL Context"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#m0-scan-information","text":"These are the fields that allow the program to understand where the M0 scan is located, and how it should be processed. The fields to fill out are: M0 Type: The nature of the M0 scan itself. Options include: Separate : The M0 scan is a separate acquisition from the ASL scan. It is not embedded within the ASL series. Included : The M0 scan is embedded within the ASL scan. Estimate : The M0 scan is not present, and in its place a numerical estimate should be used. Absent : The M0 scan is not present, and no numerical estimate should be used (not recommended). M0 Estimate: If the previous field is set to Estimate , then this field is required. It is the numerical value that will be used in place of the M0 scan in CBF quantification.","title":"M0 Scan Information"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#additional-asl-information","text":"These are fields that allow the program to understand the properties of the ASL data. The fields to fill out are: Manufacturer: The manufacturer of the scanner that acquired the data. Options include: GE Philips Siemens ASL Sequence Type: The sequence used to acquire the data. Options include: 2D EPI 3D Spiral 3D GRASE Labeling Type: The type of ASL labeling strategy used. Options include: PCASL PASL CASL Post Label Delay: For PASL scans or GE 3D Spiral sequences, the value to use is that of TI (inversion time). For (P)CASL scans, it is the time between the end of the labeling period and the start of the MRI acquisition. Labeling Duration: The duration of the labeling pulse, in milliseconds. This should be left alone at zero if the labeling type is PASL Bolus Cut-Off Flag: This is the PASL-only field that indicates whether the bolus cut-off technique was used (recommended for accurate quantification of PASL data). Bolus Cut-Off Technique: This is the PASL-only field that indicates the type of bolus cut-off technique used. Options include: QUIPSS , QUIPSSII , Q2TIPS Bolus Cut-Off Delay Time: This is the PASL-only field that indicates the timings used to define the labeling period. For Q2TIPs, the field change to 2 timings for the first and last bolus cut-off pulses, respectively.","title":"Additional ASL Information"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#background-suppression-information","text":"These are fields that assist with correcting for the effect of background suppression on the ASL data. The fields to fill out are: - Number of Background Suppression Pulses: The number of background suppression pulses present in the ASL scan. - Background Suppression Pulses Timing: The timings of the background suppression pulses, in seconds, separated by commas. Here is an example of the fields filled out for the GE \"global context\" data:","title":"Background Suppression Information"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#additional-context-specifying-subsets","text":"Each \"Additional Context\" interface has an extra field featuring a filepath dropzone for identifying which subjects/visits/sessions are to be treated with that context. Note that folders that were earlier specified as Ignore or Scan when defining the dataset structure will not be allowed in the dropzone.","title":"Additional Context - Specifying Subsets"},{"location":"Tutorial/1_Import/3_Defining_Contexts/#reloading-upon-revisit","text":"Since this step is by far the most tedious, a series of configuration files are saved at the study root folder: These will be used to auto-reload the contexts if this step is revisited for the same study. Note that this assumes that the dataset structure has not changed since the last time the step was completed. Once all the contexts have been defined, click Next in the bottom right corner to proceed to the next step.","title":"Reloading upon revisit"},{"location":"Tutorial/1_Import/4_Running_Exploreasl_Import/","text":"Running the ExploreASL Import Module This page is relatively straightforward. It is simply a text feedback field coupled with buttons to control the execution (Pause, Resume, Stop) of the import process. At the current time, the text feedback is direct output from the ExploreASL program. Simply click Run ExploreASL Import Module in the bottom right corner of the GUI to start the import process. The GUI will then look like this: And when complete, you will receive a message to take a look at the generated rawdata and derivatives folders, in particular the log files from the ExploreASL process, in order to verify that all expected images were imported over. In a future update, this part will be more automated to give better feedback regarding import errors, if any occurred. We now have a BIDS-compliant dataset: As part of the import validity verification, in the next step we will be looking at the BIDS sidecars that were generated during the import process. To proceed, toggle the navigation menu and select Edit BIDS Sidecars .","title":"Running ExploreASL Import Module"},{"location":"Tutorial/1_Import/4_Running_Exploreasl_Import/#running-the-exploreasl-import-module","text":"This page is relatively straightforward. It is simply a text feedback field coupled with buttons to control the execution (Pause, Resume, Stop) of the import process. At the current time, the text feedback is direct output from the ExploreASL program. Simply click Run ExploreASL Import Module in the bottom right corner of the GUI to start the import process. The GUI will then look like this: And when complete, you will receive a message to take a look at the generated rawdata and derivatives folders, in particular the log files from the ExploreASL process, in order to verify that all expected images were imported over. In a future update, this part will be more automated to give better feedback regarding import errors, if any occurred. We now have a BIDS-compliant dataset: As part of the import validity verification, in the next step we will be looking at the BIDS sidecars that were generated during the import process. To proceed, toggle the navigation menu and select Edit BIDS Sidecars .","title":"Running the ExploreASL Import Module"},{"location":"Tutorial/2_BIDSDataGrid/0_Overview/","text":"Overview of Editing BIDS Sidecars Purpose of this Module It should be noted that this entire module is optional, yet recommended . The import process is designed to be as robust as possible, but it is not perfect. In particular, the anonymization process may have missed some important information that is required for the processing of the data. This module allows you to manually edit the BIDS sidecars to ensure that all required information is present. Navigating to the Module To navigate to this module, toggle the navigation menu and select Edit BIDS Sidecars . You will then be greeted with the following screen:","title":"Module Overview"},{"location":"Tutorial/2_BIDSDataGrid/0_Overview/#overview-of-editing-bids-sidecars","text":"","title":"Overview of Editing BIDS Sidecars"},{"location":"Tutorial/2_BIDSDataGrid/0_Overview/#purpose-of-this-module","text":"It should be noted that this entire module is optional, yet recommended . The import process is designed to be as robust as possible, but it is not perfect. In particular, the anonymization process may have missed some important information that is required for the processing of the data. This module allows you to manually edit the BIDS sidecars to ensure that all required information is present.","title":"Purpose of this Module"},{"location":"Tutorial/2_BIDSDataGrid/0_Overview/#navigating-to-the-module","text":"To navigate to this module, toggle the navigation menu and select Edit BIDS Sidecars . You will then be greeted with the following screen:","title":"Navigating to the Module"},{"location":"Tutorial/2_BIDSDataGrid/1_Load_A_Study/","text":"Loading a Study Loading in a study is as simple as specifying the path to the study's root directory. The program will then automatically detect the BIDS structure of the study and load all the relevant information to be presented in spreadsheet form: What is loaded in? At minimum, the spreadsheet will always contain the following two columns: - ID : This is the spreadsheet's index ID. It is used to keep track of the row's position in the spreadsheet. - Filename : This is the name of the ASL BIDS sidecar that a particular row corresponds to. These two columns are essential to the functionality of the spreadsheet and thus cannot be modified or removed. The rest of the columns are automatically generated based on the information present in the BIDS sidecars. The following is a list of all the columns that can be generated: Note that the names presented here are BIDS names . The spreadsheet will display the names in a more user-friendly format. Numeric Fields BackgroundSuppressionNumberPulses BolusCutOffDelayTime EchoTime FlipAngle InversionTime LabelingDuration MagneticFieldStrength M0Estimate M0_GMScaleFactor PostLabelingDelay RepetitionTimePreparation TotalAcquiredPairs TotalReadoutTime Text Fields PulseSequenceDetails ReceiveCoilName ScanningSequence SequenceName SequenceVariant SoftwareVersions Enum Fields ArterialSpinLabelingType BolusCutOffTechnique CASLType Manufacturer M0Type MRAcquisitionType PASLType PCASLType PulseSequenceType PhaseEncodingDirection SliceEncodingDirection Boolean Fields BackgroundSuppression BolusCutOffFlag SkullStripped In future updates for BIDS fields will be included. However, for 99% of the cases, the above fields should be sufficient to use with ExploreASL.","title":"Loading a Study"},{"location":"Tutorial/2_BIDSDataGrid/1_Load_A_Study/#loading-a-study","text":"Loading in a study is as simple as specifying the path to the study's root directory. The program will then automatically detect the BIDS structure of the study and load all the relevant information to be presented in spreadsheet form:","title":"Loading a Study"},{"location":"Tutorial/2_BIDSDataGrid/1_Load_A_Study/#what-is-loaded-in","text":"At minimum, the spreadsheet will always contain the following two columns: - ID : This is the spreadsheet's index ID. It is used to keep track of the row's position in the spreadsheet. - Filename : This is the name of the ASL BIDS sidecar that a particular row corresponds to. These two columns are essential to the functionality of the spreadsheet and thus cannot be modified or removed. The rest of the columns are automatically generated based on the information present in the BIDS sidecars. The following is a list of all the columns that can be generated: Note that the names presented here are BIDS names . The spreadsheet will display the names in a more user-friendly format. Numeric Fields BackgroundSuppressionNumberPulses BolusCutOffDelayTime EchoTime FlipAngle InversionTime LabelingDuration MagneticFieldStrength M0Estimate M0_GMScaleFactor PostLabelingDelay RepetitionTimePreparation TotalAcquiredPairs TotalReadoutTime Text Fields PulseSequenceDetails ReceiveCoilName ScanningSequence SequenceName SequenceVariant SoftwareVersions Enum Fields ArterialSpinLabelingType BolusCutOffTechnique CASLType Manufacturer M0Type MRAcquisitionType PASLType PCASLType PulseSequenceType PhaseEncodingDirection SliceEncodingDirection Boolean Fields BackgroundSuppression BolusCutOffFlag SkullStripped In future updates for BIDS fields will be included. However, for 99% of the cases, the above fields should be sufficient to use with ExploreASL.","title":"What is loaded in?"},{"location":"Tutorial/2_BIDSDataGrid/2_Editing_Values/","text":"Editing BIDS Values Like Excel, the spreadsheet allows you to edit the values of the cells. Simply double-click the cell of interest and an editor view of that cell will appear with the value focused. You can then edit the value within the confines of what is reasonably permitted for that cell. For example, you cannot enter a letter in a cell that is meant for numbers. Once you are done editing the value, press Enter to save the changes or click elsewhere to have the cell lose focus and auto-submit its value. NOTE : For certain complex fields like Bolus Cut-off Delay Time that switch between a number and a collection of numbers, saving values may be somewhat slow (i.e. you press Enter and the previous value remains). This is related to the next section (validation is occurring) and performance improvements are planned. For the time being, if you encounter this, you may need to wait a second for the value to be saveable. Making Mistakes Of course, even within the confines of what is reasonably permitted for a cell, you can still make mistakes that will invalidate the integrity of the data in terms of BIDS specifications. Not to worry, though, as validation is executed as values are submitted. For instance, let us put in an unreasonable value for Magnetic Field Strength : As you can see, the cell is highlighted in red to indicate its invalidity. Furthermore, to keep track of the locations of errors for larger datasets with hundreds of rows, the option to view errors appears in the toolbar. Clicking it will open a small popover window that will list all the errors in the spreadsheet: Incompatible Fields In BIDS, there are cases where certain fields are exclusive to one another. For example, if the Arterial Spin Labeling Type is set to PASL , then there cannot exist a field of Labeling Duration for that same scan/row. But until now we only saw how to change the value of cells to other values. How do we remove a value altogether to say \"this cell should be treated as empty\"? The answer is to select the cell of interest and press the Delete key on your keyboard (not Backspace !). This will remove the value from the cell and set it to empty. NOTE : On some keyboards, this the Delete key is in the shorthand form of Del above the arrow keys.","title":"Editing Values"},{"location":"Tutorial/2_BIDSDataGrid/2_Editing_Values/#editing-bids-values","text":"Like Excel, the spreadsheet allows you to edit the values of the cells. Simply double-click the cell of interest and an editor view of that cell will appear with the value focused. You can then edit the value within the confines of what is reasonably permitted for that cell. For example, you cannot enter a letter in a cell that is meant for numbers. Once you are done editing the value, press Enter to save the changes or click elsewhere to have the cell lose focus and auto-submit its value. NOTE : For certain complex fields like Bolus Cut-off Delay Time that switch between a number and a collection of numbers, saving values may be somewhat slow (i.e. you press Enter and the previous value remains). This is related to the next section (validation is occurring) and performance improvements are planned. For the time being, if you encounter this, you may need to wait a second for the value to be saveable.","title":"Editing BIDS Values"},{"location":"Tutorial/2_BIDSDataGrid/2_Editing_Values/#making-mistakes","text":"Of course, even within the confines of what is reasonably permitted for a cell, you can still make mistakes that will invalidate the integrity of the data in terms of BIDS specifications. Not to worry, though, as validation is executed as values are submitted. For instance, let us put in an unreasonable value for Magnetic Field Strength : As you can see, the cell is highlighted in red to indicate its invalidity. Furthermore, to keep track of the locations of errors for larger datasets with hundreds of rows, the option to view errors appears in the toolbar. Clicking it will open a small popover window that will list all the errors in the spreadsheet:","title":"Making Mistakes"},{"location":"Tutorial/2_BIDSDataGrid/2_Editing_Values/#incompatible-fields","text":"In BIDS, there are cases where certain fields are exclusive to one another. For example, if the Arterial Spin Labeling Type is set to PASL , then there cannot exist a field of Labeling Duration for that same scan/row. But until now we only saw how to change the value of cells to other values. How do we remove a value altogether to say \"this cell should be treated as empty\"? The answer is to select the cell of interest and press the Delete key on your keyboard (not Backspace !). This will remove the value from the cell and set it to empty. NOTE : On some keyboards, this the Delete key is in the shorthand form of Del above the arrow keys.","title":"Incompatible Fields"},{"location":"Tutorial/2_BIDSDataGrid/3_Adding_BIDS_Fields/","text":"Adding BIDS Fields To begin adding a BIDS field that isn't currently present in the spreadsheet, click on the Add Column button located in the spreadsheet's toolbar. This will open a modal window that will prompt you to select a field to be added to the spreadsheet. For example, let's add the Skull Stipped field. After selecting a field, the appropriate input will be displayed below the field name (i.e. numeric input for numeric fields, another dropdown for enum fields, etc.). This will allow you to enter the value for the field for all the rows in the spreadsheet. Alternatively, there will be a second option presented where you can opt to populate all the rows for that column with blank values. This is particularly useful if you want to add an exclusionary field for a complex dataset where 99% of the subjects shouldn't have the field but the remaining 1% should. It would be otherwise tedious to manually delete all the values for that field. For the purposes of this tutorial, we will opt to make this \"mistake\" and populate all the rows with blank values. After pressing Add to Spreadsheet , the field will be added to the spreadsheet: NOTE This added column will always be found at the rightmost end of the spreadsheet.","title":"Adding BIDS Fields"},{"location":"Tutorial/2_BIDSDataGrid/3_Adding_BIDS_Fields/#adding-bids-fields","text":"To begin adding a BIDS field that isn't currently present in the spreadsheet, click on the Add Column button located in the spreadsheet's toolbar. This will open a modal window that will prompt you to select a field to be added to the spreadsheet. For example, let's add the Skull Stipped field. After selecting a field, the appropriate input will be displayed below the field name (i.e. numeric input for numeric fields, another dropdown for enum fields, etc.). This will allow you to enter the value for the field for all the rows in the spreadsheet. Alternatively, there will be a second option presented where you can opt to populate all the rows for that column with blank values. This is particularly useful if you want to add an exclusionary field for a complex dataset where 99% of the subjects shouldn't have the field but the remaining 1% should. It would be otherwise tedious to manually delete all the values for that field. For the purposes of this tutorial, we will opt to make this \"mistake\" and populate all the rows with blank values. After pressing Add to Spreadsheet , the field will be added to the spreadsheet: NOTE This added column will always be found at the rightmost end of the spreadsheet.","title":"Adding BIDS Fields"},{"location":"Tutorial/2_BIDSDataGrid/4_Removing_BIDS_Fields/","text":"Removing BIDS Fields Like Excel, the spreadsheet allows you to remove columns from the spreadsheet. Click on the Remove Column button located in the spreadsheet's toolbar. This will open a modal window that will prompt you to select the field(s) to be removed from the spreadsheet. For the purposes of this tutorial, we will make the mistake of removing all the fields. After acknowleding the warning and pressing Remove from Spreadsheet , the fields will be removed: Of course, in a real-world scenario, you would never want to remove all the fields from the spreadsheet. This is just to demonstrate the functionality of the spreadsheet. In the next section we will undo this mistake.","title":"Removing BIDS Fields"},{"location":"Tutorial/2_BIDSDataGrid/4_Removing_BIDS_Fields/#removing-bids-fields","text":"Like Excel, the spreadsheet allows you to remove columns from the spreadsheet. Click on the Remove Column button located in the spreadsheet's toolbar. This will open a modal window that will prompt you to select the field(s) to be removed from the spreadsheet. For the purposes of this tutorial, we will make the mistake of removing all the fields. After acknowleding the warning and pressing Remove from Spreadsheet , the fields will be removed: Of course, in a real-world scenario, you would never want to remove all the fields from the spreadsheet. This is just to demonstrate the functionality of the spreadsheet. In the next section we will undo this mistake.","title":"Removing BIDS Fields"},{"location":"Tutorial/2_BIDSDataGrid/5_Reloading_Data/","text":"Reloading Data As in the previous example, sometimes the mistakes we make are not easily reversible and a hard refresh is required. This can be done by clicking the Reload Data button in the toolbar. This will prompt the program to reload the data from the study again and will reset the spreadsheet to its original state. WARNING If, instead of clicking the Reload Data button, you were to save the changes made by clicking Save & Overwrite BIDS Sidecars , then reloading the data will not undo the changes made. This is because reloading treats the rawdata folder as the source of truth. Only click Save & Overwrite BIDS Sidecars if you are sure that your changes are what you want and are in line with BIDS standards. In a measure to be understanding that strange exceptions exist, the option to save changes will remain enabled even when there are BIDS errors present, but the responsibility in then on the user. If it all went wrong Despite the above warning, what can a user do if they saved significant changes that can't be undone easily? Chances are, it would be far more efficient to simply re-run the Import Module and start over rather than editing potentially hundreds of files manually.","title":"Reloading Data"},{"location":"Tutorial/2_BIDSDataGrid/5_Reloading_Data/#reloading-data","text":"As in the previous example, sometimes the mistakes we make are not easily reversible and a hard refresh is required. This can be done by clicking the Reload Data button in the toolbar. This will prompt the program to reload the data from the study again and will reset the spreadsheet to its original state. WARNING If, instead of clicking the Reload Data button, you were to save the changes made by clicking Save & Overwrite BIDS Sidecars , then reloading the data will not undo the changes made. This is because reloading treats the rawdata folder as the source of truth. Only click Save & Overwrite BIDS Sidecars if you are sure that your changes are what you want and are in line with BIDS standards. In a measure to be understanding that strange exceptions exist, the option to save changes will remain enabled even when there are BIDS errors present, but the responsibility in then on the user.","title":"Reloading Data"},{"location":"Tutorial/2_BIDSDataGrid/5_Reloading_Data/#if-it-all-went-wrong","text":"Despite the above warning, what can a user do if they saved significant changes that can't be undone easily? Chances are, it would be far more efficient to simply re-run the Import Module and start over rather than editing potentially hundreds of files manually.","title":"If it all went wrong"},{"location":"Tutorial/2_BIDSDataGrid/6_Hiding_Filtering_Resizing/","text":"Hiding, Filtering, and Resizing A few additional features are available for the spreadsheet, which are accessible through the spreadsheet's toolbar. These include: Hiding Columns The visibility of columns (other than ID and Filename) can be toggled on and off through the popover window that appears when you click on the Columns button in the toolbar. Filtering Rows Which rows to display on the basis of their value for a particular field can be toggled on and off through the popover window that appears when you click on the Filters button in the toolbar. Changing Row Size Finally, the overall compactness of rows can be adjusted through the popover window that appears when you click on the Density button in the toolbar. This is useful if you have a large number of rows and want to make the spreadsheet more compact. In the next section, we will learn how to define the global study & processing parameters of ExploreASL.","title":"Hiding, Filtering, and Resizing"},{"location":"Tutorial/2_BIDSDataGrid/6_Hiding_Filtering_Resizing/#hiding-filtering-and-resizing","text":"A few additional features are available for the spreadsheet, which are accessible through the spreadsheet's toolbar. These include:","title":"Hiding, Filtering, and Resizing"},{"location":"Tutorial/2_BIDSDataGrid/6_Hiding_Filtering_Resizing/#hiding-columns","text":"The visibility of columns (other than ID and Filename) can be toggled on and off through the popover window that appears when you click on the Columns button in the toolbar.","title":"Hiding Columns"},{"location":"Tutorial/2_BIDSDataGrid/6_Hiding_Filtering_Resizing/#filtering-rows","text":"Which rows to display on the basis of their value for a particular field can be toggled on and off through the popover window that appears when you click on the Filters button in the toolbar.","title":"Filtering Rows"},{"location":"Tutorial/2_BIDSDataGrid/6_Hiding_Filtering_Resizing/#changing-row-size","text":"Finally, the overall compactness of rows can be adjusted through the popover window that appears when you click on the Density button in the toolbar. This is useful if you have a large number of rows and want to make the spreadsheet more compact. In the next section, we will learn how to define the global study & processing parameters of ExploreASL.","title":"Changing Row Size"},{"location":"Tutorial/3_DataPar/0_Overview/","text":"Module Overview The bulk of the information required to process the ASL data is stored within the separate BIDS sidecars by the end of the BIDS Import module. However, there are still some \"global\" parameters that need to be defined in order for ExploreASL to know how to process the data. These parameters address concepts such as: Which subjects should be processed at runtime? Which shouldn't? What modeling assumptions should be made for the ASL data? Do we want to prioritize quality over speed when processing? Should DARTEL be used to create between-subject registration templates? Should motion correction be performed on the ASL data? If so, what T-value threshold should be used to determine which volumes should be discarded? Which atlases should be used to parcellate region-of-interest CBF data? These parameters are stored in a file called DataPar.json which, when created through this module, will be stored under the study root directory. For a more in-depth explanation of the parameters stored in DataPar.json , please refer to the ExploreASL backend's documentation website .","title":"Module Overview"},{"location":"Tutorial/3_DataPar/0_Overview/#module-overview","text":"The bulk of the information required to process the ASL data is stored within the separate BIDS sidecars by the end of the BIDS Import module. However, there are still some \"global\" parameters that need to be defined in order for ExploreASL to know how to process the data. These parameters address concepts such as: Which subjects should be processed at runtime? Which shouldn't? What modeling assumptions should be made for the ASL data? Do we want to prioritize quality over speed when processing? Should DARTEL be used to create between-subject registration templates? Should motion correction be performed on the ASL data? If so, what T-value threshold should be used to determine which volumes should be discarded? Which atlases should be used to parcellate region-of-interest CBF data? These parameters are stored in a file called DataPar.json which, when created through this module, will be stored under the study root directory. For a more in-depth explanation of the parameters stored in DataPar.json , please refer to the ExploreASL backend's documentation website .","title":"Module Overview"},{"location":"Tutorial/3_DataPar/1_StudyPars/","text":"Study Parameters This section of the module general covers filepaths and subject selection for ExploreASL to be able to perform as needed. ExploreASL and Study-Specific Parameters As in the Import Module , defining the type of ExploreASL installation, its location, and (if applicable) the location of the MATLAB Runtime installation are all required files. Furthermore, the location of the root folder of the study is also required. For controlling the name of the study in reports generated by ExploreASL, you can specify the optional Study Name field in this section. For more information on the types of ExploreASL installations refer to the section where the structures of these are explained in the Import Module . Subject Selection and Exclusion It is possible to want to only process a subset of subjects while excluding others despite all having been imported into BIDS format. For example, you may wish to exclude subjects for a re-analysis with bad quality data removed, as identified when we look interactively in the Data Visualization section. Two filepath dropzone fields are available for this purpose. One for explicit inclusion and the other for explicit exclusion . IMPORTANT NOTE When specifying the subject folders for inclusion/exclusion, remember to make the selection come from the rawdata folder, not sourcedata or derivatives . Recall that the import process to BIDS format will have altered subject folder names (i.e. appended a sub- prefix, replaced characters like hyphens, etc.).","title":"Study Parameters"},{"location":"Tutorial/3_DataPar/1_StudyPars/#study-parameters","text":"This section of the module general covers filepaths and subject selection for ExploreASL to be able to perform as needed.","title":"Study Parameters"},{"location":"Tutorial/3_DataPar/1_StudyPars/#exploreasl-and-study-specific-parameters","text":"As in the Import Module , defining the type of ExploreASL installation, its location, and (if applicable) the location of the MATLAB Runtime installation are all required files. Furthermore, the location of the root folder of the study is also required. For controlling the name of the study in reports generated by ExploreASL, you can specify the optional Study Name field in this section. For more information on the types of ExploreASL installations refer to the section where the structures of these are explained in the Import Module .","title":"ExploreASL and Study-Specific Parameters"},{"location":"Tutorial/3_DataPar/1_StudyPars/#subject-selection-and-exclusion","text":"It is possible to want to only process a subset of subjects while excluding others despite all having been imported into BIDS format. For example, you may wish to exclude subjects for a re-analysis with bad quality data removed, as identified when we look interactively in the Data Visualization section. Two filepath dropzone fields are available for this purpose. One for explicit inclusion and the other for explicit exclusion . IMPORTANT NOTE When specifying the subject folders for inclusion/exclusion, remember to make the selection come from the rawdata folder, not sourcedata or derivatives . Recall that the import process to BIDS format will have altered subject folder names (i.e. appended a sub- prefix, replaced characters like hyphens, etc.).","title":"Subject Selection and Exclusion"},{"location":"Tutorial/3_DataPar/2_SeqPars/","text":"Default ASL Sequence & Modeling Parameters Sequence & Acquisition Parameters Deprecation Warning : This is a legacy section that will be removed in the next release. Originally, ExploreASL did not incorporate BIDS format into its analysis, and so the many ASL sequence & acquisition related parameters that were identified in the Import Module were originally specified in this section. Note that this section can still be used to specify fallback values in the event that the import procedure did not extract relevant information from the DICOM headers (i.e. due to excessive anonymization or human errors in scan acquisition). However, it is more recommended to adjust settings at the scan level using the \"Edit BIDS Sidecars\" module discussed in an earlier section . Modeling and CBF Quantification Parameters This section is responsible for specifying the modeling and quantification parameters for extracting CBF values from the ASL data. Please refer to the ASL consensus paper , for which the default values shown above are set from (assuming 3T strength acquisition) or the backend ExploreASL documentation for more information on these parameters.","title":"Default ASL Sequence & Modeling Parameters"},{"location":"Tutorial/3_DataPar/2_SeqPars/#default-asl-sequence-modeling-parameters","text":"","title":"Default ASL Sequence &amp; Modeling Parameters"},{"location":"Tutorial/3_DataPar/2_SeqPars/#sequence-acquisition-parameters","text":"Deprecation Warning : This is a legacy section that will be removed in the next release. Originally, ExploreASL did not incorporate BIDS format into its analysis, and so the many ASL sequence & acquisition related parameters that were identified in the Import Module were originally specified in this section. Note that this section can still be used to specify fallback values in the event that the import procedure did not extract relevant information from the DICOM headers (i.e. due to excessive anonymization or human errors in scan acquisition). However, it is more recommended to adjust settings at the scan level using the \"Edit BIDS Sidecars\" module discussed in an earlier section .","title":"Sequence &amp; Acquisition Parameters"},{"location":"Tutorial/3_DataPar/2_SeqPars/#modeling-and-cbf-quantification-parameters","text":"This section is responsible for specifying the modeling and quantification parameters for extracting CBF values from the ASL data. Please refer to the ASL consensus paper , for which the default values shown above are set from (assuming 3T strength acquisition) or the backend ExploreASL documentation for more information on these parameters.","title":"Modeling and CBF Quantification Parameters"},{"location":"Tutorial/3_DataPar/3_ProcPars/","text":"ExploreASL Processing Parameters This section will cover control over the different behaviours of submodules within the ExploreASL pipeline, which include: Structural Module; for processing anatomical data into a template space ASL Module; for processing ASL & M0 data into CBF volumes Population Module; for generating population statistics from the processed data General Processing Parameters These are meta parameters that generally affect processing time as well as memory & disk usage. Processing & Image Quality - Controls the quality and precision of the processing. An approximate 5-fold increase to the processing time is expected when using the higher quality setting (~20-30 min for a single subject with a single visit & scan). Handling of Folders & Missing Scans - Self-explanatory. The default settings (see below) are to save on disk space but avoid skipping scans where at least some partial processing can be done. Structural Module Parameters See the ExploreASL documentation for more information on these parameters. ASL Module Parameters See the ExploreASL documentation for more information on these parameters. Population Module Parameters See the ExploreASL documentation for more information on these parameters. IMPORTANT NOTE If no atlases are specified, then it will not be possible to use the Data Visualization module of this software, as no CBF ROI statistics will be generated. Ensure that you research which atlas(es) are important for your study and check what is appropriate.","title":"ExploreASL Processing Parameters"},{"location":"Tutorial/3_DataPar/3_ProcPars/#exploreasl-processing-parameters","text":"This section will cover control over the different behaviours of submodules within the ExploreASL pipeline, which include: Structural Module; for processing anatomical data into a template space ASL Module; for processing ASL & M0 data into CBF volumes Population Module; for generating population statistics from the processed data","title":"ExploreASL Processing Parameters"},{"location":"Tutorial/3_DataPar/3_ProcPars/#general-processing-parameters","text":"These are meta parameters that generally affect processing time as well as memory & disk usage. Processing & Image Quality - Controls the quality and precision of the processing. An approximate 5-fold increase to the processing time is expected when using the higher quality setting (~20-30 min for a single subject with a single visit & scan). Handling of Folders & Missing Scans - Self-explanatory. The default settings (see below) are to save on disk space but avoid skipping scans where at least some partial processing can be done.","title":"General Processing Parameters"},{"location":"Tutorial/3_DataPar/3_ProcPars/#structural-module-parameters","text":"See the ExploreASL documentation for more information on these parameters.","title":"Structural Module Parameters"},{"location":"Tutorial/3_DataPar/3_ProcPars/#asl-module-parameters","text":"See the ExploreASL documentation for more information on these parameters.","title":"ASL Module Parameters"},{"location":"Tutorial/3_DataPar/3_ProcPars/#population-module-parameters","text":"See the ExploreASL documentation for more information on these parameters. IMPORTANT NOTE If no atlases are specified, then it will not be possible to use the Data Visualization module of this software, as no CBF ROI statistics will be generated. Ensure that you research which atlas(es) are important for your study and check what is appropriate.","title":"Population Module Parameters"},{"location":"Tutorial/3_DataPar/4_Saving_And_Loading/","text":"Saving and Loading Configurations To save or load a DataPar.json configuration file, click the Save or Load button located at the bottom of ExploreASL-GUI's main window. Saving After filling in the relevant parameters and clicking the Save button, one of two things will happen: The save does not occur and invalid fields are marked throughout with red highlights. This is to ensure that the user does not accidentally save an invalid configuration file. Fix these as prompted by the error message hints and try again. The save occurs and you receive a confirmation message: We can examine the contents of the configuration file by opening it in a text editor: And in the event that there is a limitation to the GUI, it is possible to manually edit the fields in. This is not recommended, but can be done if necessary. For more information on the appropriate values to use in each field in the event of manual configuration, please refer to the ExploreASL documentation . Loading It is likely that in the course of your work, you will need to tweak certain parameters in your configuration file. Re-entering all of the parameters manually can be tedious, so it is possible to load a previously saved configuration file and edit the parameters as necessary. As with saving, after clicking the Load button and selecting the dataPar.json file located at the study root folder, one of two things will happen: The load occurs without warning and all fields are populated with the values from the configuration file. The load still occurs, but due to certain fields being invalid (i.e. perhaps BIDS schema has changed, filepaths are no longer valid, etc.), you will be prompted with a warning message detailing which fields were invalid and the particular validation error encountered. In the example below, filepath fields for the ExploreASL directory and the study root directory were deliberately tampered with to demonstrate the warning message: With the data imported, validated, and global configurations define, we can finally proceed to run the main ExploreASL analysis.","title":"Saving and Loading Configuations"},{"location":"Tutorial/3_DataPar/4_Saving_And_Loading/#saving-and-loading-configurations","text":"To save or load a DataPar.json configuration file, click the Save or Load button located at the bottom of ExploreASL-GUI's main window.","title":"Saving and Loading Configurations"},{"location":"Tutorial/3_DataPar/4_Saving_And_Loading/#saving","text":"After filling in the relevant parameters and clicking the Save button, one of two things will happen: The save does not occur and invalid fields are marked throughout with red highlights. This is to ensure that the user does not accidentally save an invalid configuration file. Fix these as prompted by the error message hints and try again. The save occurs and you receive a confirmation message: We can examine the contents of the configuration file by opening it in a text editor: And in the event that there is a limitation to the GUI, it is possible to manually edit the fields in. This is not recommended, but can be done if necessary. For more information on the appropriate values to use in each field in the event of manual configuration, please refer to the ExploreASL documentation .","title":"Saving"},{"location":"Tutorial/3_DataPar/4_Saving_And_Loading/#loading","text":"It is likely that in the course of your work, you will need to tweak certain parameters in your configuration file. Re-entering all of the parameters manually can be tedious, so it is possible to load a previously saved configuration file and edit the parameters as necessary. As with saving, after clicking the Load button and selecting the dataPar.json file located at the study root folder, one of two things will happen: The load occurs without warning and all fields are populated with the values from the configuration file. The load still occurs, but due to certain fields being invalid (i.e. perhaps BIDS schema has changed, filepaths are no longer valid, etc.), you will be prompted with a warning message detailing which fields were invalid and the particular validation error encountered. In the example below, filepath fields for the ExploreASL directory and the study root directory were deliberately tampered with to demonstrate the warning message: With the data imported, validated, and global configurations define, we can finally proceed to run the main ExploreASL analysis.","title":"Loading"},{"location":"Tutorial/4_RunEASL/0_Overview/","text":"Module Overview To navigate to this module, toggle ExploreASL-GUI's navigation bar and click the Run ExploreASL option found under Process Studies . You should be greeted with the following screen: This module is responsible for running one or more studies with defined dataPar.json files & imported scans simultaneously. By default, the module is set to process only a single study, but you can change this by selecting the appropriate value under the Select the numer of studies to process dropdown menu. For example, by selecting 5 from the dropdown menu, you will be able to process 5 studies simultaneously: Warning The number of studies you can process simultaneously is limited by the number of CPU cores available on your machine. For example, if you have a 4-core CPU, you will only be able to process 4 studies simultaneously. Attempting to exceed this may result in a crash. Logic to prevent this from happening is in place, but due diligence is still advised. For this tutorial, we will only be processing the single study we imported.","title":"Module Overview"},{"location":"Tutorial/4_RunEASL/0_Overview/#module-overview","text":"To navigate to this module, toggle ExploreASL-GUI's navigation bar and click the Run ExploreASL option found under Process Studies . You should be greeted with the following screen: This module is responsible for running one or more studies with defined dataPar.json files & imported scans simultaneously. By default, the module is set to process only a single study, but you can change this by selecting the appropriate value under the Select the numer of studies to process dropdown menu. For example, by selecting 5 from the dropdown menu, you will be able to process 5 studies simultaneously: Warning The number of studies you can process simultaneously is limited by the number of CPU cores available on your machine. For example, if you have a 4-core CPU, you will only be able to process 4 studies simultaneously. Attempting to exceed this may result in a crash. Logic to prevent this from happening is in place, but due diligence is still advised. For this tutorial, we will only be processing the single study we imported.","title":"Module Overview"},{"location":"Tutorial/4_RunEASL/1_RunEASL/","text":"Processing a Study Run Congifuration Opening one of the \"accordion\" panels will reveal the following view: As suggested by the headings, the panel is subdivided into 3 sections: Study Settings , where you can specify which study to process, how many resources (CPU cores) to allocate, and which ExploreASL module to run in the first place. Run Control , where you can initiate the processing of the study, pause it, resume from pause, or cancel it. This section also displays the current progress completion of the pipeline. Progam Feedback , much like in running the Import Module , feedback is given to the user as to the progress of the pipeline. Note that the feedback is more succinct, as the pipeline generates a lot of log statements that would otherwise clutter the program feedback panel. In addition, when certain quality-control steps complete, axial and coronal slices of the processed data are displayed in the panel as images. Study Settings The fields in this section are as follows: Study Root Path : The root path of the study to process. This is the folder that contains the dataPar.json file generate in the previous module. Number of Cores Allocated for this Study : Self-explanatory. Note that certain numbers may be disabled if the program detects that another study already has a certain number of cores allocated to it. Modules to Run : The particular ExploreASL module to run. Options include: Strucural Module ASL Module Structural & ASL Module Population Module NOTE If selecting the lattermost, you may notice the run button is greyed out. This is probably due to the fact that only a single core can be allocated to running a Population Module. Running State Once you have specified the study root path, the ExploreASL module to run, and have clicked Run Study , the panel will change to the following view: At this point in time, you cannot alter the study configuration or change the number of studies to process. This is, again, for the sake of preventing over-extension of your workstations resources and to prevent the pipeline from crashing. As the pipeline progresses, you will be able to observe that under the derivatives/ExploreASL/lock folder, there are folders and files being generated that correspond to the current state of the pipeline. As checkpoints are passed in the pipeline, certain .status files are created to indicate the passing of that checkpoint. Warning If you manipulate this lock structure or the .status files, you may cause errors in the pipeline. This is purely for your awareness and is not a feature that is intended to be used by the user. What if something went wrong during an earlier step? Before running, the program re-validates the dataPar.json file and the rawdata folder in order to minimize the chance of a user-based error propagating into the analysis pipeline. If you attempt to run a study with an invalid setup, you will be greeted with an error message explaining the problem. For example, if we attempt to run the tampered dataPar.json file from the [DataPar Module](../3_DataPar/4_Saving_And_Loading.md#loading, we will be greeted with the following error message: Completing a Run If a run is completed successfully, you will be greeted with the following message: And you will be able to either start a new run on another module, or move onto Data Visualization . Alternatively, you may encounter an error during the run. If this is the case, you will be greeted with the following message: As you can see, the error message features an overview of the number of steps that failed and a summary of the earlier step that failed for which subject/visit/session. From here, it is up to the user to review the log files found under derivatives/ExploreASL/logs detailing exactly what went wrong or to review their dataset to see if there are corrupt files or other issues that may have caused the pipeline to fail. Completing a Study Analysis A study is considered analyzed when all 3 modules (Structural, ASL, Population) have been successfully completed. It is not possible to perform the next module, Data Visualization, for a particular study until all 3 modules have been completed.","title":"Process a Study"},{"location":"Tutorial/4_RunEASL/1_RunEASL/#processing-a-study","text":"","title":"Processing a Study"},{"location":"Tutorial/4_RunEASL/1_RunEASL/#run-congifuration","text":"Opening one of the \"accordion\" panels will reveal the following view: As suggested by the headings, the panel is subdivided into 3 sections: Study Settings , where you can specify which study to process, how many resources (CPU cores) to allocate, and which ExploreASL module to run in the first place. Run Control , where you can initiate the processing of the study, pause it, resume from pause, or cancel it. This section also displays the current progress completion of the pipeline. Progam Feedback , much like in running the Import Module , feedback is given to the user as to the progress of the pipeline. Note that the feedback is more succinct, as the pipeline generates a lot of log statements that would otherwise clutter the program feedback panel. In addition, when certain quality-control steps complete, axial and coronal slices of the processed data are displayed in the panel as images.","title":"Run Congifuration"},{"location":"Tutorial/4_RunEASL/1_RunEASL/#study-settings","text":"The fields in this section are as follows: Study Root Path : The root path of the study to process. This is the folder that contains the dataPar.json file generate in the previous module. Number of Cores Allocated for this Study : Self-explanatory. Note that certain numbers may be disabled if the program detects that another study already has a certain number of cores allocated to it. Modules to Run : The particular ExploreASL module to run. Options include: Strucural Module ASL Module Structural & ASL Module Population Module NOTE If selecting the lattermost, you may notice the run button is greyed out. This is probably due to the fact that only a single core can be allocated to running a Population Module.","title":"Study Settings"},{"location":"Tutorial/4_RunEASL/1_RunEASL/#running-state","text":"Once you have specified the study root path, the ExploreASL module to run, and have clicked Run Study , the panel will change to the following view: At this point in time, you cannot alter the study configuration or change the number of studies to process. This is, again, for the sake of preventing over-extension of your workstations resources and to prevent the pipeline from crashing. As the pipeline progresses, you will be able to observe that under the derivatives/ExploreASL/lock folder, there are folders and files being generated that correspond to the current state of the pipeline. As checkpoints are passed in the pipeline, certain .status files are created to indicate the passing of that checkpoint. Warning If you manipulate this lock structure or the .status files, you may cause errors in the pipeline. This is purely for your awareness and is not a feature that is intended to be used by the user.","title":"Running State"},{"location":"Tutorial/4_RunEASL/1_RunEASL/#what-if-something-went-wrong-during-an-earlier-step","text":"Before running, the program re-validates the dataPar.json file and the rawdata folder in order to minimize the chance of a user-based error propagating into the analysis pipeline. If you attempt to run a study with an invalid setup, you will be greeted with an error message explaining the problem. For example, if we attempt to run the tampered dataPar.json file from the [DataPar Module](../3_DataPar/4_Saving_And_Loading.md#loading, we will be greeted with the following error message:","title":"What if something went wrong during an earlier step?"},{"location":"Tutorial/4_RunEASL/1_RunEASL/#completing-a-run","text":"If a run is completed successfully, you will be greeted with the following message: And you will be able to either start a new run on another module, or move onto Data Visualization . Alternatively, you may encounter an error during the run. If this is the case, you will be greeted with the following message: As you can see, the error message features an overview of the number of steps that failed and a summary of the earlier step that failed for which subject/visit/session. From here, it is up to the user to review the log files found under derivatives/ExploreASL/logs detailing exactly what went wrong or to review their dataset to see if there are corrupt files or other issues that may have caused the pipeline to fail.","title":"Completing a Run"},{"location":"Tutorial/4_RunEASL/1_RunEASL/#completing-a-study-analysis","text":"A study is considered analyzed when all 3 modules (Structural, ASL, Population) have been successfully completed. It is not possible to perform the next module, Data Visualization, for a particular study until all 3 modules have been completed.","title":"Completing a Study Analysis"},{"location":"Tutorial/4_RunEASL/2_ReRun/","text":"Rerunning A Study As with re-defining the dataPar.json file, sometimes it is necessary to re-run the ExploreASL pipeline. To do so, it is first necessary to indicate the modules/subjects/sessions/steps that are to be re-run. This can be achieved by clicking on the Prepare a Re-Run tab located at the top of this module. You will be greeted with an input field for the study root folder for which you'd like to perform a re-run on. Note You cannot re-run a study that is currently being processed. If you attempt to do so, you will be greeted with the following error message: Once you have specified the study root folder (and verified that the study is not currently being processed), you will be greeted with the following view: The \"tree-view\" items here directly correspond to the folders/files found under derivatives/ExploreASL/lock . To indicate a re-run, simply select the items that you'd like to have re-evaluated and click the button at the bottom of the screen. You can then proceed back to the Run ExploreASL tab and re-run that study.","title":"Re-run a Study"},{"location":"Tutorial/4_RunEASL/2_ReRun/#rerunning-a-study","text":"As with re-defining the dataPar.json file, sometimes it is necessary to re-run the ExploreASL pipeline. To do so, it is first necessary to indicate the modules/subjects/sessions/steps that are to be re-run. This can be achieved by clicking on the Prepare a Re-Run tab located at the top of this module. You will be greeted with an input field for the study root folder for which you'd like to perform a re-run on. Note You cannot re-run a study that is currently being processed. If you attempt to do so, you will be greeted with the following error message: Once you have specified the study root folder (and verified that the study is not currently being processed), you will be greeted with the following view: The \"tree-view\" items here directly correspond to the folders/files found under derivatives/ExploreASL/lock . To indicate a re-run, simply select the items that you'd like to have re-evaluated and click the button at the bottom of the screen. You can then proceed back to the Run ExploreASL tab and re-run that study.","title":"Rerunning A Study"},{"location":"Tutorial/5_DataViz/0_Overview/","text":"Module Overview Purpose Although ExploreASL exports excellent reports as to the processing of a particular subject/visit/session/scan, it can often be tedious to manually inspect each of these reports to determine the overall quality of the data. This module serves to address these limitations by providing an interactive plot of the data, from which individual perfusion (and interactive) volumes can be loaded in and inspected for significant assistance with quality control. The following is an example of the module's output: While not immediately evident due to the picture being static, the above plot is, in-fact, interactive. The mouse was hovering over a datapoint, rendering a tooltip with information pertaining to that datapoint, including the subject/visit/session information. Furthermore, when the datapoint was clicked, the corresponding perfusion volume was loaded in and displayed below the plot. Each perfusion volume was also interactive. If the mouse hovered over a particular voxel, the a tooltip with the voxel's value and coordinates would render. In the manner, the module allows for: - Immediate visual inspection of the data, including the ability to identify outliers - The ability to quickly load in individual perfusion volumes for further inspection and know exactly what other data is associated with that volume Pre-requisites Study Status It is assumed that you have a fully-analyzed study by the time you need to perform quality control assessment. Metadata/Ancillary Information Incorporation Furthermore, if you wish to incorporate your own ancillary data (i.e. clinical, demographics, etc.) into the assessment, you will need to have this data merge-able with the output of ExploreASL. The example below demonstrates what is required for the example dataset: Your metadata file must be saved as a .csv or .tsv file. Excel is NOT supported at the current time. This is simple enough to ameliorate by going to File > Save As in Excel and selecting the appropriate format. The metadata must have at least one column called participant_id which contains the subject/visit identifiers in the same format as found in the derivatives/ExploreASL folder. Do not use subject folders from rawdata or sourcedata , as ExploreASL alters these names during its own BIDS2Legacy submodule. (OPTIONAL; REQUIRED ONLY IF MULTIPLE SESSIONS EXIST PER VISIT) The metadata may also contain a column called session . These values must be the same as the session folders found in derivatives/ExploreASL/{SUBJECT_VISIT} . Typically these are ASL_1 , ASL_2 , etc. If you do not have multiple sessions per visit, you can ignore this column. In the example above, this is indeed the case: we have multiple visits (as seen by the _1 and _2 suffixes found under participant_id ), but only one session per visit. If the above is satisfied (or if you don't have any metadata to incorporate), you can proceed to the next section.","title":"Module Overview"},{"location":"Tutorial/5_DataViz/0_Overview/#module-overview","text":"","title":"Module Overview"},{"location":"Tutorial/5_DataViz/0_Overview/#purpose","text":"Although ExploreASL exports excellent reports as to the processing of a particular subject/visit/session/scan, it can often be tedious to manually inspect each of these reports to determine the overall quality of the data. This module serves to address these limitations by providing an interactive plot of the data, from which individual perfusion (and interactive) volumes can be loaded in and inspected for significant assistance with quality control. The following is an example of the module's output: While not immediately evident due to the picture being static, the above plot is, in-fact, interactive. The mouse was hovering over a datapoint, rendering a tooltip with information pertaining to that datapoint, including the subject/visit/session information. Furthermore, when the datapoint was clicked, the corresponding perfusion volume was loaded in and displayed below the plot. Each perfusion volume was also interactive. If the mouse hovered over a particular voxel, the a tooltip with the voxel's value and coordinates would render. In the manner, the module allows for: - Immediate visual inspection of the data, including the ability to identify outliers - The ability to quickly load in individual perfusion volumes for further inspection and know exactly what other data is associated with that volume","title":"Purpose"},{"location":"Tutorial/5_DataViz/0_Overview/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"Tutorial/5_DataViz/0_Overview/#study-status","text":"It is assumed that you have a fully-analyzed study by the time you need to perform quality control assessment.","title":"Study Status"},{"location":"Tutorial/5_DataViz/0_Overview/#metadataancillary-information-incorporation","text":"Furthermore, if you wish to incorporate your own ancillary data (i.e. clinical, demographics, etc.) into the assessment, you will need to have this data merge-able with the output of ExploreASL. The example below demonstrates what is required for the example dataset: Your metadata file must be saved as a .csv or .tsv file. Excel is NOT supported at the current time. This is simple enough to ameliorate by going to File > Save As in Excel and selecting the appropriate format. The metadata must have at least one column called participant_id which contains the subject/visit identifiers in the same format as found in the derivatives/ExploreASL folder. Do not use subject folders from rawdata or sourcedata , as ExploreASL alters these names during its own BIDS2Legacy submodule. (OPTIONAL; REQUIRED ONLY IF MULTIPLE SESSIONS EXIST PER VISIT) The metadata may also contain a column called session . These values must be the same as the session folders found in derivatives/ExploreASL/{SUBJECT_VISIT} . Typically these are ASL_1 , ASL_2 , etc. If you do not have multiple sessions per visit, you can ignore this column. In the example above, this is indeed the case: we have multiple visits (as seen by the _1 and _2 suffixes found under participant_id ), but only one session per visit. If the above is satisfied (or if you don't have any metadata to incorporate), you can proceed to the next section.","title":"Metadata/Ancillary Information Incorporation"},{"location":"Tutorial/5_DataViz/1_Loading_Data/","text":"Loading a Dataset In order for a dataset to be loaded, the following must be specified: Study Root Path : The filepath to the folder for this analyzed study. This is the folder that, itself, contains the derivatives , rawdata , and sourcedata folders. Metadata Filepath : (Optional) The filepath to the .csv or .tsv file containing additional data to merge with the output of ExploreASL. See the Metadata/Ancillary Information Incorporation section for more information. Which Atlas Statistic to Load : Self-explanatory - are we interested in the mean, median, or spatial coefficient of variation (CoV) of the data? Which Partial Volume Correction Spreadsheet to Load : Self-explanatory - are we interested in the partial volume corrected (PVC) or non-partial volume corrected (non-PVC) data? Atlases to Load : Which regions of interest based on atlas do we want to have CBF values loaded from. Note that you can only load the atlases that were analyzed and indicated earlier when we were defining ExploreASL's processing settings . If performed correctly, clicking on Proceed to Clarify Data Types will proceed to the next step.","title":"Loading in a Study"},{"location":"Tutorial/5_DataViz/1_Loading_Data/#loading-a-dataset","text":"In order for a dataset to be loaded, the following must be specified: Study Root Path : The filepath to the folder for this analyzed study. This is the folder that, itself, contains the derivatives , rawdata , and sourcedata folders. Metadata Filepath : (Optional) The filepath to the .csv or .tsv file containing additional data to merge with the output of ExploreASL. See the Metadata/Ancillary Information Incorporation section for more information. Which Atlas Statistic to Load : Self-explanatory - are we interested in the mean, median, or spatial coefficient of variation (CoV) of the data? Which Partial Volume Correction Spreadsheet to Load : Self-explanatory - are we interested in the partial volume corrected (PVC) or non-partial volume corrected (non-PVC) data? Atlases to Load : Which regions of interest based on atlas do we want to have CBF values loaded from. Note that you can only load the atlases that were analyzed and indicated earlier when we were defining ExploreASL's processing settings . If performed correctly, clicking on Proceed to Clarify Data Types will proceed to the next step.","title":"Loading a Dataset"},{"location":"Tutorial/5_DataViz/2_Changing_DTypes/","text":"Clarifying Data Types When the data has successfully loaded, you will be presented with a table of column names and their detected data types. Data type options include: Categorical (or nominal/ordinal data) Continuous (or interval/ratio/numerical data) Ignore (to exclude a column from further analysis) Once you have clarified the data types, you can proceed to the next step by clicking on Go to Plotting in the bottom right corner of the window. Categorical Encoded Data While it is likely that the majority of the interpretation of columns will be accurate, it is quite often the case that some categorical data may be mis-interpreted by the program as continuous since it is encoded as numbers. By clarifying this earlier, we can allow the program to use these numerically-encoded columns for plots that have a categorical x-axis (i.e. swarmplots).","title":"Clarifying Data Types"},{"location":"Tutorial/5_DataViz/2_Changing_DTypes/#clarifying-data-types","text":"When the data has successfully loaded, you will be presented with a table of column names and their detected data types. Data type options include: Categorical (or nominal/ordinal data) Continuous (or interval/ratio/numerical data) Ignore (to exclude a column from further analysis) Once you have clarified the data types, you can proceed to the next step by clicking on Go to Plotting in the bottom right corner of the window.","title":"Clarifying Data Types"},{"location":"Tutorial/5_DataViz/2_Changing_DTypes/#categorical-encoded-data","text":"While it is likely that the majority of the interpretation of columns will be accurate, it is quite often the case that some categorical data may be mis-interpreted by the program as continuous since it is encoded as numbers. By clarifying this earlier, we can allow the program to use these numerically-encoded columns for plots that have a categorical x-axis (i.e. swarmplots).","title":"Categorical Encoded Data"},{"location":"Tutorial/5_DataViz/3_Plot_Overview/","text":"Plotting and Interactivity Overview Upon data type clarification, the plotting module will fully open and you will be greeted with the following view: The plotting module is divided into three main sections: - The main plot area - The plot controls - The MRI views (seen below the main plot area) Plot Controls These 4 panels govern the behavior of this module. In summary: - Plot Variables , where you specify the type of plot to be displayed (scatterplot for continuous x continuous, swarmplot for categorical x continuous, etc.) - Subsetting Settings , where you can subset the data by several criteria in order to focus on a specific series of subjects/visits/sessions/scans - MRI View Controls , for controlled the slices displayed for a loaded MRI volume - Swarm Plot Visuals or Scatter Plot Visuals , where you can personalize the appearance of the plot (i.e. labels, grid line thickness, etc.) Plot Variables The Plot Variables panel controls the nature of the plot and the data that is displayed. Several inputs are available: Graph Type : Scatterplot : Continuous x Continuous data analysis Swarmplot : Categorical x Continuous data analysis X Axis : Which column to use for the x-axis. Selections are dependent on the Graph Type selected. If the Graph Type is Scatterplot , then the X Axis can be any continuous data type column. If the Graph Type is Swarmplot , then the X Axis can be any categorical data type column. Y Axis : Which column to use for the y-axis. For the time being, this can only be a continuous data type column, as both plot types use continuous values for their dependent (y) variable representation. Color : An optional column that is only available if the Graph Type is Scatterplot . This column is expected to be categorical data that will separate data points into different color groups, the levels of which will be displayed in a legend generated on the right side of the plot. Additional Hover Data : As the plots are interactive, you can hover over data points to see their not only the x and y values, but also any additional columns that you specify here. By default, if this field is left blank, the hover data that is displayed over each data point will include: The Subject/Visit ID The Session ID The X and Y values The color group (if applicable) The remaining plot controls will be covered in their own sections (see below). For now, let's proceed to rendering a basic scatterplot. Scatter Plot Visuals Swarm Plot Visuals MRI View Controls Subsetting Settings","title":"Plotting and Interactivity Overview"},{"location":"Tutorial/5_DataViz/3_Plot_Overview/#plotting-and-interactivity-overview","text":"Upon data type clarification, the plotting module will fully open and you will be greeted with the following view: The plotting module is divided into three main sections: - The main plot area - The plot controls - The MRI views (seen below the main plot area)","title":"Plotting and Interactivity Overview"},{"location":"Tutorial/5_DataViz/3_Plot_Overview/#plot-controls","text":"These 4 panels govern the behavior of this module. In summary: - Plot Variables , where you specify the type of plot to be displayed (scatterplot for continuous x continuous, swarmplot for categorical x continuous, etc.) - Subsetting Settings , where you can subset the data by several criteria in order to focus on a specific series of subjects/visits/sessions/scans - MRI View Controls , for controlled the slices displayed for a loaded MRI volume - Swarm Plot Visuals or Scatter Plot Visuals , where you can personalize the appearance of the plot (i.e. labels, grid line thickness, etc.)","title":"Plot Controls"},{"location":"Tutorial/5_DataViz/3_Plot_Overview/#plot-variables","text":"The Plot Variables panel controls the nature of the plot and the data that is displayed. Several inputs are available: Graph Type : Scatterplot : Continuous x Continuous data analysis Swarmplot : Categorical x Continuous data analysis X Axis : Which column to use for the x-axis. Selections are dependent on the Graph Type selected. If the Graph Type is Scatterplot , then the X Axis can be any continuous data type column. If the Graph Type is Swarmplot , then the X Axis can be any categorical data type column. Y Axis : Which column to use for the y-axis. For the time being, this can only be a continuous data type column, as both plot types use continuous values for their dependent (y) variable representation. Color : An optional column that is only available if the Graph Type is Scatterplot . This column is expected to be categorical data that will separate data points into different color groups, the levels of which will be displayed in a legend generated on the right side of the plot. Additional Hover Data : As the plots are interactive, you can hover over data points to see their not only the x and y values, but also any additional columns that you specify here. By default, if this field is left blank, the hover data that is displayed over each data point will include: The Subject/Visit ID The Session ID The X and Y values The color group (if applicable) The remaining plot controls will be covered in their own sections (see below). For now, let's proceed to rendering a basic scatterplot. Scatter Plot Visuals Swarm Plot Visuals MRI View Controls Subsetting Settings","title":"Plot Variables"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/","text":"Scatterplot Basic Scatter Plot At minimum, to render a scatterplot, you must specify the following: - Graph Type : Scatterplot - X Axis : Any continuous data type column - Y Axis : Any continuous data type column The following is an example of a basic scatterplot: Initially, not very pretty, but certainly useable for basic data exploration. Let's see how we can make this plot more visually appealing. Scatter Plot Visual Settings The Scatter Plot Visuals panel allows you to adjust the appearance of the scatterplot. There are a considerable number of options available which will be summarized below: Color Color Palette: Allows for control over the color palette used when a Color column is specified in the Plot Variables panel. The default is Set 1 , but you can also select from a number of other known palettes. Markers Size: Allows for control over the scatter point marker size. Margins Left, Top, Right, Bottom: Allows for control over the margins of the plot. This is useful if you want to increase the size of the plot to make it more readable or if other elements are overlapping with the plot. Grid Lines X Axis, Y Axis: Whether to render grid lines on the x and y axes, respectively. Grid Line Width: The thickness of the grid lines. X Axis Settings X Axis Label: The label for the x axis. Defaults to the column label, but this allows you to override it for a more readable label. Tick Height: The length of the ticks protruding from the x axis. Tick Width: The thickness of the ticks protruding from the x axis. Tick Label Offset: The distance from the x axis to the tick labels. Tick Label Rotation: The angle of rotation of the tick labels. This is useful if the tick labels are overlapping with each other. Tick Label Font Size: The font size of the tick labels. Axis Label Font Size: The font size of the main x axis label/title. Axis Label Offset: The distance from the x axis to the main x axis label/title. Y Axis Settings Same as the x axis settings, but for the y axis. Legend Settings Main Anchor: The anchor point for the legend. This is useful if you want to move the legend to a different location on the plot. Legend Direction: The orientation of items within the legend. Item Packing Order: The order of items within the legend. Translate X: The horizontal offset of the legend from the anchor point. Translate Y: The vertical offset of the legend from the anchor point. Item Spacing: The spacing between items within the legend. Legend Item Width: The width of each item (marker + label) within the legend. Legend Item Height: The height of each item (marker + label) within the legend. Symbol Size: The size of the markers within the legend. Label Font Size: The font size of the labels within the legend. Prettified Scatter Plot Let's see how we can make the scatterplot look a little nicer once the visual settings are adjusted:","title":"Scatterplot"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#scatterplot","text":"","title":"Scatterplot"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#basic-scatter-plot","text":"At minimum, to render a scatterplot, you must specify the following: - Graph Type : Scatterplot - X Axis : Any continuous data type column - Y Axis : Any continuous data type column The following is an example of a basic scatterplot: Initially, not very pretty, but certainly useable for basic data exploration. Let's see how we can make this plot more visually appealing.","title":"Basic Scatter Plot"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#scatter-plot-visual-settings","text":"The Scatter Plot Visuals panel allows you to adjust the appearance of the scatterplot. There are a considerable number of options available which will be summarized below:","title":"Scatter Plot Visual Settings"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#color","text":"Color Palette: Allows for control over the color palette used when a Color column is specified in the Plot Variables panel. The default is Set 1 , but you can also select from a number of other known palettes.","title":"Color"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#markers","text":"Size: Allows for control over the scatter point marker size.","title":"Markers"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#margins","text":"Left, Top, Right, Bottom: Allows for control over the margins of the plot. This is useful if you want to increase the size of the plot to make it more readable or if other elements are overlapping with the plot.","title":"Margins"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#grid-lines","text":"X Axis, Y Axis: Whether to render grid lines on the x and y axes, respectively. Grid Line Width: The thickness of the grid lines.","title":"Grid Lines"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#x-axis-settings","text":"X Axis Label: The label for the x axis. Defaults to the column label, but this allows you to override it for a more readable label. Tick Height: The length of the ticks protruding from the x axis. Tick Width: The thickness of the ticks protruding from the x axis. Tick Label Offset: The distance from the x axis to the tick labels. Tick Label Rotation: The angle of rotation of the tick labels. This is useful if the tick labels are overlapping with each other. Tick Label Font Size: The font size of the tick labels. Axis Label Font Size: The font size of the main x axis label/title. Axis Label Offset: The distance from the x axis to the main x axis label/title.","title":"X Axis Settings"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#y-axis-settings","text":"Same as the x axis settings, but for the y axis.","title":"Y Axis Settings"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#legend-settings","text":"Main Anchor: The anchor point for the legend. This is useful if you want to move the legend to a different location on the plot. Legend Direction: The orientation of items within the legend. Item Packing Order: The order of items within the legend. Translate X: The horizontal offset of the legend from the anchor point. Translate Y: The vertical offset of the legend from the anchor point. Item Spacing: The spacing between items within the legend. Legend Item Width: The width of each item (marker + label) within the legend. Legend Item Height: The height of each item (marker + label) within the legend. Symbol Size: The size of the markers within the legend. Label Font Size: The font size of the labels within the legend.","title":"Legend Settings"},{"location":"Tutorial/5_DataViz/4_Scatter_Plot/#prettified-scatter-plot","text":"Let's see how we can make the scatterplot look a little nicer once the visual settings are adjusted:","title":"Prettified Scatter Plot"},{"location":"Tutorial/5_DataViz/5_Swarm_Plot/","text":"Swarmplot In general, swarmplots are used to visualize the distribution of a continuous variable across a categorical variable. Swarmplot Visuals In general, the same settings are available for swarmplots as for scatterplots. The primary differences lie in a few additional marker settings: Border Width: The width of the border around each data point Between-Groups Spacing: The spacing between each level of the categorical variable plotted on the x-axis Swarmplot Prettified The following is an example of a swarmplot with the adjusted display settings:","title":"Swarmplot"},{"location":"Tutorial/5_DataViz/5_Swarm_Plot/#swarmplot","text":"In general, swarmplots are used to visualize the distribution of a continuous variable across a categorical variable.","title":"Swarmplot"},{"location":"Tutorial/5_DataViz/5_Swarm_Plot/#swarmplot-visuals","text":"In general, the same settings are available for swarmplots as for scatterplots. The primary differences lie in a few additional marker settings: Border Width: The width of the border around each data point Between-Groups Spacing: The spacing between each level of the categorical variable plotted on the x-axis","title":"Swarmplot Visuals"},{"location":"Tutorial/5_DataViz/5_Swarm_Plot/#swarmplot-prettified","text":"The following is an example of a swarmplot with the adjusted display settings:","title":"Swarmplot Prettified"},{"location":"Tutorial/5_DataViz/6_MRI_View/","text":"Interactive MRI View As mentioned in the module's overview, clicking interactively with the plot's data points will load in the corresponding perfusion MRI volume for that data point. Do take note that clicking on several datapoints in quick succession (i.e. <2-3 seconds apart) will consume a lot of memory, so it is recommended to space clicks apart liberally. This is usually the case anyways, as you will likely want to inspect the MRI volume throughout in order to verify the absence of artifacts, distortions, vascular effects, etc. This MRI view is itself interactive, such that hovering over particular voxels will display the corresponding perfusion values and x/y location within the plane. Of course, the MRI view is not restricted to one plane across the three dimensional views. You can move along a plane using the MRI View Controls panel.","title":"Interactive MRI View"},{"location":"Tutorial/5_DataViz/6_MRI_View/#interactive-mri-view","text":"As mentioned in the module's overview, clicking interactively with the plot's data points will load in the corresponding perfusion MRI volume for that data point. Do take note that clicking on several datapoints in quick succession (i.e. <2-3 seconds apart) will consume a lot of memory, so it is recommended to space clicks apart liberally. This is usually the case anyways, as you will likely want to inspect the MRI volume throughout in order to verify the absence of artifacts, distortions, vascular effects, etc. This MRI view is itself interactive, such that hovering over particular voxels will display the corresponding perfusion values and x/y location within the plane. Of course, the MRI view is not restricted to one plane across the three dimensional views. You can move along a plane using the MRI View Controls panel.","title":"Interactive MRI View"},{"location":"Tutorial/5_DataViz/7_Data_Subsetting/","text":"Data Subsetting In a similar manner to the BIDS editing spreadsheet, this module also allows you to subset your loaded data by several criteria. This is particularly useful for larger datasets for better performance as well as to focus on a specific collection of datapoints with greater focus. Subsetting Settings To create a subsetter, click on the Add a new Subsetter button located in the Subsetting Settings panel of the Plot Controls drawer, and follow the steps below: You will first be prompted to indicate the column by which subsetting criteria will be generated. Depending the the data type of the column, the subsetting operator (i.e. == , != , > , < , includes , excludes , etc.) options will be loaded in. You will then be prompted to enter the value(s) by which the subsetting criteria will be generated. For categorical data, this will be a dropdown menu of the unique values in the column. For continuous data, this will be a numerical input field. Once all of these are entered, the plot will automatically update to reflect the subsetted data. In the example below, we subsetted the data by Age to only include subjects with an age greater than 45.","title":"Data Subsetting"},{"location":"Tutorial/5_DataViz/7_Data_Subsetting/#data-subsetting","text":"In a similar manner to the BIDS editing spreadsheet, this module also allows you to subset your loaded data by several criteria. This is particularly useful for larger datasets for better performance as well as to focus on a specific collection of datapoints with greater focus.","title":"Data Subsetting"},{"location":"Tutorial/5_DataViz/7_Data_Subsetting/#subsetting-settings","text":"To create a subsetter, click on the Add a new Subsetter button located in the Subsetting Settings panel of the Plot Controls drawer, and follow the steps below: You will first be prompted to indicate the column by which subsetting criteria will be generated. Depending the the data type of the column, the subsetting operator (i.e. == , != , > , < , includes , excludes , etc.) options will be loaded in. You will then be prompted to enter the value(s) by which the subsetting criteria will be generated. For categorical data, this will be a dropdown menu of the unique values in the column. For continuous data, this will be a numerical input field. Once all of these are entered, the plot will automatically update to reflect the subsetted data. In the example below, we subsetted the data by Age to only include subjects with an age greater than 45.","title":"Subsetting Settings"},{"location":"Tutorial/5_DataViz/8_Exporting/","text":"Exporting Plot and Images Finally, it may be easily missable that there is an option in the lower right corner of the plot, Export Figure To PNG , which will allow you to save the plot and the MRI views as a single figure in a PNG file. A file dialog will open to allow you to specify the location and name of the file, where it will then be saved. Concluding Remarks This marks the end of the tutorial. Hopefully it has been a useful introduction to the capabilities of ExploreASL-GUI and the various ways in which it can be used to analyze your ASL data. For commonly asked questions that may not have been covered in this tutorial, please refer to the FAQ page. For contacting the ExploreASL team, please refer to the Contact page.","title":"Exporting Plots"},{"location":"Tutorial/5_DataViz/8_Exporting/#exporting-plot-and-images","text":"Finally, it may be easily missable that there is an option in the lower right corner of the plot, Export Figure To PNG , which will allow you to save the plot and the MRI views as a single figure in a PNG file. A file dialog will open to allow you to specify the location and name of the file, where it will then be saved.","title":"Exporting Plot and Images"},{"location":"Tutorial/5_DataViz/8_Exporting/#concluding-remarks","text":"This marks the end of the tutorial. Hopefully it has been a useful introduction to the capabilities of ExploreASL-GUI and the various ways in which it can be used to analyze your ASL data. For commonly asked questions that may not have been covered in this tutorial, please refer to the FAQ page. For contacting the ExploreASL team, please refer to the Contact page.","title":"Concluding Remarks"}]}